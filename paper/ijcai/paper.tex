\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai19}
% The file ijcai18.sty is the style file for IJCAI-18 (same as ijcai08.sty).

\input{packages}
\input{macros}

% the following package is optional:
\usepackage{latexsym}

\title{Demystifying the Combination of Dynamic Slicing and \\ Spectrum-based Fault
Localization}

% Single author syntax
\author{
    Submission 6364
}
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\hypersetup{draft}
\begin{document}

\maketitle

\begin{abstract}
Several approaches have been proposed in the literature to improve debugging
costs through automated software fault diagnosis.  Dynamic Slicing (\ds{}) and
Spectrum-based Fault Localization (\sfl{}) are very popular fault diagnosis
techniques and normally seen as complementary.  This paper reports the outcome
of a study using real-world applications with the goal of demystifying the
impact of combining \ds{} with \sfl{}.  We suspect that several highly
suspicious components---often involved in failing, but seldom in passing
executions---may be unrelated to the bug. With the aforementioned combination,
such components could be identified and their suspiciousness reduced.  This
study also discusses the impact of the dynamic slicing key limitation---the risk
of missing faulty statements. Our results show that, in practice, \ds{} rarely
misses faulty statements (23 misses in 260 cases) and that the
\ds{}-\sfl{} combination, coined as Tandem-SL, improves the diagnostic accuracy
in 51\% of the faults. To sum, we found that, despite tacit opinions of the
research community about the usefulness of \ds{} in automated debugging, we
found the combination practical and effective, and encourage new \sfl{}
techniques to be evaluated against that optimization.
\end{abstract}


\section{Introduction}

Software debugging is important and challenging. In particular, the task of locating the faulty
code (\ie{}, fault localization) is particularly challenging. For that reason,
countless automated techniques have been proposed in the past to reduce the cost
of fault localization~\cite{7390282}.

%% Although the technique has been intensively investigated in
%% research, few use cases gained notoriety---WhyLine~\cite{Ko:2008:DRA:1368088.1368130} being an
%% exception.


Model-based software diagnosis (MBSD)~\cite{REITER198757,DEKLEER200325} and Spectrum-Based Fault Localization
(\sfl{}) are two very popular techniques that leverage different principles
to automate fault isolation. An example of a MBSD is Dynamic
Slicing (DS)~\cite{Agrawal:1990:DPS:93542.93576} as well as some work~\cite{wotawa2002model,Mayer:2008:EMM:1642931.1642950,mayer2008prioritising,Perez:2018:LQR:3304889.3304927},
a technique that traces back statements in the code that influence some 
point of interest, such as the evaluation
of a failing assertion. In contrast to \ds{}, \sfl{}~\cite{7390282}
does not analyze code. Instead, it is a black-box, statistical fault
localization method that computes suspiciousness values associated
with program entities (e.g., methods) based on coverage information gathered
during the execution of test cases as well as the
output of test cases. More precisely, \sfl{} uses coverage information
of passing and failing test cases to identify likely faulty
statements and produces on output a list of program entities ranked in
decreasing order of suspiciousness.
\ds{} and \sfl{} are complementary debugging techniques~\cite{DBLP:conf/sac/AbreuGZG08}.
Intuitively, \ds{} identifies irrelevant parts of the code whereas \sfl{}
ranks the relevant parts of the code.

%% Similarly to \ds{}, \sfl{} received
%% tremendous attention in research over the years,
%% but its applicability to support
%% debugging remains
%% questionable~\cite{ang-perez-van-deursen-rui-2017,Pearson:2017:EIF:3097368.3097441,Xie:2016:RAD:2884781.2884834}.
%% Despite the skepticism of the research community, \sfl{} has been
%% shown useful in supporting downstream analyses, such as Automated
%% Program Repair
%% (\apr{})~\cite{automatic-software-repair-survey2017,kim-etal-daghstul2017},
%% an increasingly popular technique that looks for fixes to buggy
%% statements. Tools like JAFF \cite{arcuri-2011}, Prophet
%% \cite{long-rinard-2016}, SemFix \cite{nguyen-qi-roychoudhury-2013},
%% and SPR \cite{long-rinard-2015} use \sfl{} to guide the search for
%% likely fixes.

%% We found surprising that, despite these findings, no tool or client analyses use this combination
%% today.\Sof{@todo: Revise this and look to the paper from 2018} Several reasons could justify that observation. One hypothesis is that results reported in prior work are
%% over-optimistic. For example, most prior work evaluated improvements
%% of \sfl{} techniques using relative metrics, which are based on the
%% position of the first faulty statements found in the ranking relative
%% to the total number of ranked statements, which is often a large
%% number. As such, it inflates actual improvements and deceives
%% potential adopters of the technology. Ang \etal~\cite{ang-perez-van-deursen-rui-2017} recently
%% pointed to that fact and encouraged researchers to adopt more precise metrics,
%% such as \topk{}
%% \cite{Wu:2014:CLC:2610384.2610386,Lucia:2014:FFL:2642937.2642983,Wen:2016:LLB:2970276.2970359},
%% which has been widely adopted to evaluate performance of information retrieval algorithms~\Fix{cite}. This
%% metric reports the percentage of faults captured by a technique when
%% the rank is trimmed to the first $k$ components.

This paper reports the results of a a study using real-world applications to
evaluate the impact of combining \ds{} and \sfl{} to improve software fault
localization. We coin the toolset materializing this approach as \comb{}.  The
intuition for the combination is that several highly ranked statements, albeit
covered by failing executions, may be unrelated to the fault. Our empirical
study involves \numFaults{} faults from \numPrograms{} different programs
comprised in the Defects4J (D4J) dataset~\cite{just-defects4j-issta2014}, which
is frequently used to evaluate fault localization research. Prior work reported
promising results in this
combination~\cite{Wotawa:2010:FLB:1848650.1849235,Alves:2011:FUD:2190078.2190115,DBLP:conf/ecai/HoferW12,lei-mao-dai-wang-2012,slicing-sfl-repair},
but they used a non-representative set of subjects in their evaluation or used
archaic methods to evaluate technique
improvement~\cite{Wu:2014:CLC:2610384.2610386,Lucia:2014:FFL:2642937.2642983,Wen:2016:LLB:2970276.2970359}.

The goal of this paper is to reassess whether the \comb{} approach
yields promising results under a more rigorous experimental
methodology and a larger dataset.\comments{The D4J dataset is about
  twice as large than the biggest test suite used in previous
  studies.} This study covers the following research questions:

\newcommand{\rqone}{How often does DS miss faulty statements?}
\newcommand{\rqthree}{How effective is the \comb{} approach?}

\begin{itemize}[leftmargin=*]
\item[]{\footnotesize[(un)soundness]}~\textit{\rqone{}}
\item[]{\footnotesize[effectiveness]}~\textit{\rqthree{}}
\end{itemize}

The first question addresses the important problem of missing faulty
statements. Naturally, the consequence of that problem for debugging
is that developers would never be able to locate faults. Different
slicers can miss faulty statements for different reasons. In this
paper, we used Critical Slicing~\cite{DeMillo:1996:CSS:229000.226310},
which, in principle, could miss statements because of imprecise
oracles that guide the slicing process. We evaluated how often that
issue manifests in our experiments. The second research questions
effectively evaluates the impact of the combination. Our
results show that \Sof{@todo: sum up the results}.

The contributions of this work are the following ones:
\begin{itemize}
	\item A study, using real-world applications and bugs, on the combination
    of \ds{} and \sfl{} for bug localization of \emph{Java} faulty programs.
   \item A tool that implements \comb{}.
\end{itemize}

The tool as well as a replication package will be available once double-blind
requirements are lifted.

\section{Background}
\label{sec:background}

This section summarizes the ground concepts of our approach.

\subsection{Dynamic Slicing (\ds{})}
\label{sec:slicing}

Program slicing is a program understanding technique to identify the
relevant parts of the program with respect to given points of
interest. Dynamic
slicing~\cite{Agrawal:1990:DPS:93542.93576}\comments{~--~as opposed to
static slicing~\cite{Weiser:1981:PS:800078.802557}~--~~\cite{Binkley:2014:OLP:2635868.2635893}}
has been shown useful in automated software debugging where the region
of interest is restricted to what can be reached from failing
tests. Several dynamic slicing techniques exist. This paper uses
Critical Slicing (\cs{})~\cite{DeMillo:1996:CSS:229000.226310} for its
simplicity/generality. Critical Slicing
prescribes a black-box language-semantics-agnostic recipe to computing
executable slices.  Critical Slicing simplifies the original
program such that the resulting program preserves critical
observations, such as assertion violations. More precisely, the
simplification mechanism consists of deleting statements on the
original program and checking if the output of the original and
modified program are the same.

Our implementation of Critical Slicing is based on the Mozilla Lithium
tool\footnote{Lithium details available at \url{https://github.com/MozillaSecurity/lithium} 
(Accessed on \today)}. It takes as input a file
and produces as output a simplified version of that file that
satisfies a user-defined oracle. In our case, the oracle is defined
such that the test produces the same failure manifestation as the one
observed with the test execution on the original program. The
Lithium minimization process starts by determining the initial size---in 
number of lines---of chunks to delete from the input file. For that,
it chooses the highest power of two number smaller than the file
size. For example, if the file has 1,000 lines, Lithium sets the
initial chunk size to 512 lines. Then, the tool starts a local search looking
for chunks to exclude from the file. If the chunk satisfies the oracle it is removed.
When no more chunks of that given size can be removed, Lithium
divides the chunk size by two and repeats the search. This iterative
process continues until no more lines can be removed.  If $n$ is the
size of the input file and $m$ is the size of the 1-minimal file found
by Lithium, then Lithium usually performs $O(m\cdot\lg(n))$
iterations. Proofs of the algorithm complexity can be found
elsewhere\footnote{Lithium complexity available at \url{https://github.com/MozillaSecurity/lithium/blob/master/src/lithium/docs/algorithm.md} (Accessed on \today)}. Our implementation is publicly
available at \textit{$<$anonymized for double-blind review$>$}.

\subsection{Spectrum-based Fault Localization (\sfl)}
\label{sec:sfl}

\begin{wrapfigure}[8]{2}{0.3\textwidth}
  \hspace{-2ex}
  \centering
  \scriptsize
  \begin{tabular}{c|cccc|c}
    $\mathcal{T}$ & $c_1$    & $c_2$    & $\cdots$ & $c_M$    & $e$    \\ \hline
    $t_1$         & $\A{11}$ & $\A{12}$ & $\cdots$ & $\A{1M}$ & $e_1$  \\
    $t_2$         & $\A{21}$ & $\A{22}$ & $\cdots$ & $\A{2M}$ & $e_2$  \\
    \vdots        & \vdots   & \vdots   & $\ddots$ & \vdots   & \vdots \\
    $t_N$         & $\A{N1}$ & $\A{N2}$ & $\cdots$ & $\A{NM}$ & $e_N$  \\
  \end{tabular}
  \caption{An example spectrum.}
  \label{fig:spectrum-example}
\end{wrapfigure}

Spectrum-based fault localization is a statistical fault
localization technique that takes as input a test suite including at
least one failing test and reports on output a ranked list of
components likely to be in
fault~\cite{FLSurvey2016,DBLP:conf/kbse/JonesH05,DBLP:journals/smr/LuciaLJTB14,DBLP:journals/jss/AbreuZGG09}. The
following are given in \sfl{}: a finite set $\mathcal{C}=\set{c_1,c_2,...,c_M}$
of $M$ system \emph{components}\footnote{A
component can be any code artifact of arbitrary granularity
such as a class, a method, or a statement~\cite{DBLP:journals/stvr/HarroldRSWY00}.}; 
a finite set $\mathcal{T} = \set{t_1,t_2,...,t_N}$ of $N$ system transactions,
which correspond to records of a system execution, such as test cases;
the error vector $e = \set{e_1,e_2,...,e_N}$, where $e_i = 1$ if
transaction $t_i$ has failed and $e_i = 0$ otherwise; and an
$N\times{}M$ coverage matrix $\mathcal{A}$, where $\A{ij}$ denotes the
coverage of component $c_j$ in transaction $t_i$.  The pair
$(\mathcal{A},e)$ is commonly referred to as
spectrum~\cite{DBLP:journals/stvr/HarroldRSWY00}. Figure~\ref{fig:spectrum-example}
shows an example spectrum.

Several types of spectra exist.  The most commonly used is called
hit-spectrum, where the coverage matrix is encoded in terms of binary
\emph{hit} (1) and \emph{not hit} (0) flags, \ie{}, $\A{ij} = 1$ if
$t_i$ covers $c_j$ and $\A{ij} = 0$ otherwise.  \sfl{} takes as input
the pair $(\mathcal{A},e)$ and produces on output a list of components
ranked by their faulty suspiciousness. To that end, the first step of
the technique consists of determining what columns of the matrix $A$
resemble the error vector $e$ the most.  For that, an intermediate
component frequency aggregator $n_{pq}(j)$ is computed $n_{pq}(j) =
|\{i\mid \A{ij}=p \wedge e_i=q\}|$. $n_{pq}(j)$ denotes the number of
runs in which the component $j$ has been active during execution ($p =
1$) or not ($p=0$), and in which the runs failed ($q = 1$) or passed
($q = 0$).  For instance, $n_{11}(j)$ counts the number of times
component $j$ has been involved ($p = 1$) in failing executions ($q =
1$), whereas $n_{10}(j)$ counts the number of times component $j$ has
been involved in passing executions. We then calculate similarity to
the error vector by means of applying \emph{fault predictors} to each
component to produce a score quantifying how likely it is to be
faulty.  Components are then ranked according to such likelihood
scores and reported to the user. Ochiai is one of those fault
predictors that has shown to perform
well~\cite{7390282,Pearson:2017:EIF:3097368.3097441}. The Ochiai
formula is given by the following equation
$\textit{ochiai}\,$=$\,\N{11}/\sqrt{(\N{11}+\N{01}) * (\N{11}+\N{10})}$.
%The Ochiai formula is given by~\cite{DBLP:conf/prdc/AbreuZG06}
%\[ochiai = \frac\]

\section{The \comb{} Approach}
\label{sec:approach}

This section provides details on the proposed approach, \combpar{}, that combines
\ds{} with \sfl{}; it also discusses the oracle problem.

\subsection{Combining \ds{} and \sfl{}}
\label{sec:comb}

To illustrate the idea behind the combination of \sfl{} with \ds{}, consider a
debugging scenario with five components $c_{1..5}$ and five
transactions $t_{1..5}$, two of which are failing. Consider,
additionally, that the component $c_2$ is faulty.
Figure~\ref{fig:illustration} shows, at the left-hand side of the
arrow ($\Rightarrow$), an hypothetical spectra and its corresponding
ranking, produced with the Ochiai predictor. Each line in the ranking
shows, respectively, the rank/position, the component label, and the
Ochiai score (in parentheses).

\begin{figure}[t!]

  \centering
  \begin{subfigure}{0.5\textwidth}
    {\def\arraystretch{0.9}\setlength{\tabcolsep}{3pt}
      \begin{tabular}{c|ccccc|c}
        $\mathcal{T}$ & $c_1$    & $c_2$   & $c_3$ & $c_4$ &  $c_5$   & $e$    \\ \hline
        $t_1$         & 1 & 0 & 1 & 1 & 0 &\cmark  \\
        $t_2$         & 0 & 1 & 1 & 1 & 1 &\xmark  \\
        $t_3$         & 1 & 0 & 1 & 0 & 0 &\xmark  \\
        $t_4$         & 0 & 1 & 0 & 0 & 1 &\cmark  \\
        $t_5$         & 1 & 0 & 0 & 1 & 1 &\cmark \\
        \hline
      \end{tabular}
      \quad
      $\Rightarrow$
      \quad
      \begin{tabular}{c|ccccc|c}
        $\mathcal{T}$ & $c_1$    & $c_2$   & $c_3$ & $c_4$ &  $c_5$   & $e$    \\ \hline
        $t_1$         & 1 & 0 & 1 & 1 & 0 &\cmark  \\
        $t_2$         & 0 & 1 & {\cellcolor{Gray} 0} & {\cellcolor{Gray} 0} & 1 &\xmark  \\
        $t_3$         & 1 & 0 & 1 & 0 & 0 &\xmark  \\
        $t_4$         & 0 & 1 & 0 & 0 & 1 &\cmark  \\
        $t_5$         & 1 & 0 & 0 & 1 & 1 &\cmark  \\
        \hline
      \end{tabular}
    }
    \caption{Spectra update.}
    \label{fig:ds-reduction}
  \end{subfigure}

  \vspace{2ex}

  \begin{subfigure}{0.5\textwidth}
    \centering
    \begin{tabular}{cccccc}
      1 & $c_4$ (0.59) & & 1 & $c_2$ (0.35) \\
      2 & $c_3$ (0.55) & & 2.5 & $c_1$ (0.32)\\
      3 & $c_2$ (0.35) & \hspace{1ex}$\Rightarrow$\hspace{1ex} & 2.5 & $c_5$ (0.32)\\
      4.5 & $c_1$ (0.32) & & 4 & $c_3$ (0.29)\\
      4.5 & $c_5$ (0.32) & & 5 & $c_4$ (0.00)\\
    \end{tabular}
    \caption{Ranking update.}
  \end{subfigure}

  \caption{Modifications on spectra and ranking as result of slicing
    code against test $t_2$. Double arrow ($\Rightarrow$) indicates
    before and after.}
  \label{fig:illustration}
  \vspace{-2ex}
\end{figure}

In the following, we illustrate the workflow of the \comb{}
approach for this scenario. \comb{} uses a granularity of statements.
First, the combined technique picks statements $c_4$ and $c_3$, which
appear at the top of the ranking, for further analysis.
(The cutoff point is user-defined.) Second, the
technique picks a failing test $t_2$ to slice the code. A slice is a
set of statements. Let us assume
that the slice obtained for the transaction $t_2$ is $\{c_2, c_5\}$,
\ie, it excludes statements $c_4$ and $c_3$. Finally, the spectra and
ranking are updated. The right-side of Figure~\ref{fig:illustration}
shows the modified spectra and ranking after slicing test $t_2$
against $c_4$ and $c_3$. It is worth noting that the technique slices
the code for every failing tests. More precisely, the workflow of the
\comb{} approach consists of four steps:
\begin{enumerate}
  \itemsep0em
  \item\label{step:spectra} Compute spectra $S$ and ranking $R$ for
  the input test suite;
  \item\label{step:susp-files} Select top $k$ most suspicious classes, according to $R$;
  \item\label{step:compute-slice} Compute slicer for every suspicious
    file, obtained in Step~\ref{step:susp-files}, and every failing test;
  \item\label{step:group} Assemble all the resulting slicers of each failing test;
  \item Adjust the spectra $S$, obtained in Step~\ref{step:spectra}, with
    the slices of each failing test obtained in Step~\ref{step:group} and 
	recompute ranking.
\end{enumerate}

Intuitively, slicing enables the identification of statements in the spectra
whose values can be set to zero, \ie{}, the components marked with $0$
are irrelevant to determine the test output. Let us now observe the
effect of this spectra modification on the ranking. Analyzing the
Ochiai formula, one observes that, for the components $j$ which are
not part of the slice of a failing test, the combination reduces the
value of $\N{11}$ and increases the value of $\N{01}$. Therefore,
suspiciousness of those components decrease. Similar argument applies
to other fault predictors. In the running example,
Figure~\ref{fig:illustration}, components $c_3$ and $c_4$, which are
not in the slice of $t_2$, have their suspiciousness reduced, enabling
the faulty component $c_2$ to rise from the third to the first
position in the ranking.

\begin{theorem}\label{the:1}
  The faulty statement may not be included in the critical slice of
  failing test cases.
\end{theorem}

Conceptually, if the oracles implemented in faulty tests are too
general, the critical slicing algorithm could produce slices without
the faulty code. For example, consider the scenario of an unexpected
Null-Pointer Exception (NPE) thrown in the code. If the oracle checks
for the presence of NPE regardless of where it occurs, it is possible,
in principle, that the final slice does not include the faulty code
but still raises NPE (in a different part of the code). The critical
slicing algorithm would consider that as an acceptable simplification
and would continue. Section~\ref{sec:oracleheuristics} shows how we
mitigate this fundamental limitation of Critical Slicing and
Section~\ref{sec:fault-misses} shows how problematic is that issue in
practice.

%% \vspace{3mm}
%% \begin{myframe} \label{box:1}
%% \textbf{java.lang.NullPointerException}, \\
%% at org.jfree.data.general.junit.DatasetUtilitiesTests.\\
%% testBug2849731$\_$3(DatasetUtilitiesTests.java:1299), \\
%% ...
%% \end{myframe}

\begin{theorem}
  The rank of faulty statements cannot decrease if the slice includes
  the faulty statements.
\end{theorem}

The proof is based on the outlined argument that irrelevant
components $j$ have their ranks reduced as $\N{11}$ decreases and
$\N{01}$ increases. If those irrelevant components appear at positions
above the faulty component, it is possible that the faulty component
becomes more suspicious relative to $j$, \ie{}, the ranking of the
faulty component increases. If those components appear at positions
below the faulty components, the ranking of the faulty components is
unaffected.


\subsection{Oracle Heuristics}
\label{sec:oracleheuristics}

% \subsection{Implementation}
% \label{sec:impl}

% \Mar{Sofia, I think this was way too low-level for intro and decided
%   to move here. please check how to fit.}
% \Fix{
% A couple of different tools were designed to perform this empirical study: \morpho{} and \lithium{}. \morpho{} retrieves as an output the input for \lithium{}. \lithium{} uses the input to reduce the search domain for \sfl{} and outputs the statements that resulted from the minimization. \morpho{} uses this to update the spectrum matrix, performs the before and after \sfl{} evaluation using the corresponding matrix, and outputs the metrics that report the \sfl{} performance for both cases ~---~ before and after using \ds{}. }

% Two different tools were developed to support this research: \morpho{} and \lithium{}. \morpho{} was designed to calculate the suspiciousness of all statements of a project before, and after the top-k minimization performed by \lithium{} whereas \lithium{} is responsible for reducing the search domain of each \texttt{Java} class of the project for the after-\sfl{} analysis. \morpho{} uses the spectrum matrix (Figure \ref{fig:spectrum-example}), and the pair \emph{name\#location} for each statement to calculate the respective ranking. All rankings are ordered from highest to lowest ranked. This information is retrieved along with each test case stack trace in a \texttt{.json} file which serves as an output to \lithium{}. \lithium{} starts by generating the inputs (Algorithm \ref{alg:ls}, line $2$) for the top-k classes of each failing test based, mainly, on the output of \emph{morpho} ~---~ ranked list of statements and the test cases stacktraces. Then, the tool iterates each class from the top-k classes ($c$) of each test $t$. In each iteration, classes are refined using an external java program (Algorithm \ref{alg:ls}, line 6) that substitutes all the line comments ($\backslash\backslash$), block comments ($\backslash*$ to $*\backslash$) and javadoc comments ($/**$ to $*/$) using the \texttt{JavaParser}\footnote{JavaParser is available at http://javaparser.org/ (accessed November 2018).} library. This step was added to \lithium{} because it turns \texttt{MozillaLithium} faster since the empty lines are ignored. Then, \texttt{MozillaLithium} performs the class minimization using a function of interest (Algorithm \ref{alg:finc}) which compares the output of the test with the expected one which is given as an input. Finally, the location of all relevant statements (Algorithm \ref{alg:ls}, line 9) is saved in a \texttt{.json} which is used for the before-\sfl{} analysis. \morpho{} uses the output from \lithium{} to create a copy of the older spectrum matrix and updates it according to the explanation provided on Figure \ref{fig:ds-reduction} where the statements that are not in the slice of the test suffer a suspiciouness reduction. In the end, \morpho{} performs the \sfl{} analysis for both matrixes and calculates the probability of the first line being faulty, the probability of the last line being faulty, and the mean and median of the position of the faulty line in the ranking. These are the metrics used to evaluate how considerable is the improvement obtained when combining \ds{} with \sfl{}.

% \begin{algorithm}[h]
% 	\caption{Class Minimization Algorithm}
% 	\label{alg:ls}
% 	\begin{flushleft}
% 		\textbf{Input:} $proj$ - project name \\
% 		\hspace{2.75em} $bug$ - bug number\\
% 		\hspace{2.75em} $k$ - number of top ranked classes\\
% 		\hspace{2.75em} $stk$ - expected stacktrace\\
% 	 \textbf{Output:} Top-k classes minimization for each test \\
% 	\end{flushleft}
% 	\begin{algorithmic}[1]
% 		\Procedure{lithium-slicer}{$proj$, $bug$, $k$, $stk$}
% 			\State $testsInfo \leftarrow$ generateInputs($proj$, $bug$, $k$, $stk$)
% 			\ForAll {$t \in testsInfo$}
% 				\State $classes \leftarrow$ getClasses($proj$, $bug$)
% 				\ForAll {$c \in classes$}
% 					\State $unc \leftarrow$ removeComments($c$)
% 					\State $min \leftarrow$ MozillaLithium($iFunc$, $unc$, $t$, $stk$)
% 				\EndFor
% 				\State $slicer \leftarrow$ getLocation($c$, $t$, $min$)
% 			\EndFor
% 			\State \Return $slicer$
% 		\EndProcedure
% 	\end{algorithmic}
%
% \end{algorithm}
%
% \begin{figure*}[t]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{figures/lithium.pdf}
% 	\caption{Simple illustration of the process to obtain the top-5 minimized classes from a project carrying a bug}
% \end{figure*}
%
% \subsection{Function of Interest }
% \label{sec:funcofint}
%
%

The \cs{} technique is somewhat based on the \textit{statement deletion} mutant
operator (SSDL) of the mutation-based testing methodology. This operator
was designed based on the idea that each statement has an effect on the program
output (i.e., oracle). Thus, testers are encouraged to design test sets that cause
all statements to be executed and generate outputs that are different from the
program under test.

Mozilla's Lithium takes as input a function of interest that can be designed according
to the problem to be solved. The function of interest determines if the test case output 
is interesting or not. Our function simply compares the failing test output with the output 
obtained when re-running again the failing test after deleting the slice, as explained in 
Section~\ref{sec:slicing}. Thus, if the
initial test output is the same as the one after removing the slice \{$c_2$,$c_5$\}, then 
the slice is removed. This means that the slice is not necessary to reveal the fault because 
if removed the test output is still the same.
The output is extracted from the stacktrace obtained through the failing test execution. 
The stacktrace provides information not only about the place where the fault was revelead 
but also the path taken to get there. Thus, our approach considers not only the exception 
caught by the failing test but also the path that was taken to reveal the fault. The following 
provides an example:

\vspace{3mm}
\begin{myframe} \label{box:1}
junit.framework.AssertionFailedError, \\
at junit.framework.Assert.fail(Assert.java:55), \\
at junit.framework.Assert.assertTrue(Assert.java:22), \\
at junit.framework.Assert.assertFalse(Assert.java:39), \\
at junit.framework.Assert.assertFalse(Assert.java:47), \\
at junit.framework.TestCase.assertFalse(TestCase.java:219), \\
at org.jfree.chart.util.junit.ShapeUtilitiesTests.\\testEqualGeneralPaths(ShapeUtilitiesTests.java:212), \\
...
\end{myframe}

Considering some of the stacktrace information about the path taken to reveal the fault 
solved the issue of generic oracles (e.g., NPE or generic AssertionFailedError expections 
as the one presented before) mentioned in Theorem~\ref{the:1}. There are faults whose outputs 
could slightly differ from one execution to another. For example, exceptions including information 
of memory addresses. For these cases, it was developed a mechanism to compare the rest of the output 
ignoring the identifier of the memory address. This heuristic also handles StackOverflow oracles.




%
% This information can be deducted from the defects stack traces which show the stack of functions called until the thrown of an exception. The interesting function compares both stack traces (from the test under evaluation and the expected one) until the call where the test fails. For each chunk under evaluation, the test is executed after the chunk being deleted from the source code . Both test ($testStk$) and expected ($expStk$) stack traces are filtered. We only consider stack traces until the line where the main exception is launched. These operations are performed by line 3 and 4 from Algorithm 2. Finally, both messages are compared and if equal the chunk will be deleted from the project, i.e., the chunk is not interesting to reveal the fault of the test.


\section{Evaluation}
\label{sec:eval}

To evaluate our approach, we compare the cost of diagnosing
a collection of faults using \sfl{} versus \comb{}.

\subsection{Objects of Analysis}\label{sec:analysis}

We used the \dfj{} benchmark in our
experiments~\cite{just-defects4j-issta2014}. This benchmark includes
six different subjects and \numFaults{} faults (Table \ref{tab:df4j}).
\lang{} is a library that provides a set of helper utilities for the
     {\small\texttt{java.lang}} API. \cmath{} is a lightweight library
     of self-contained mathematics and statistics components. The
     \closure{} is a toolset for turning JavaScript files into smaller
     scripts for faster download and execution in the
     browser. \chart{} is a Java library for creating charts. \jtime{}
     is a lightweight library that aims to replace the default Java
     {\small\texttt{java.util.Date}} classes providing simpler
     APIs. \mockito{} is a mocking testing framework.

\newcommand{\cgray}[1]{\cellcolor{gray!25}#1}
\begin{table}[h]
  \centering
  \setlength{\tabcolsep}{4pt}
    \begin{tabular}{lrrr}
      \toprule
     Project            & Size (LOC) & \# Tests & \# Faults \\ %\comments{& Failing Test Cases &}
      \midrule
      \lang{}            & 111,751  & 6,057 & 65       \\   %\commentst{& 124   &  -}\\
      \cmath{}           & 306,276  & 26,797 & 106     \\   %\comments{& 177   &  -}\\
      \closure{}         & 149,521  & 27,930  & 133     \\   %\comments{& 350   &  -}\\
      \chart{}           & 230,159  & 8,458 & 26      \\  %\comments{& 92    &  -}\\
      \jtime{}           & 141,610  & 3,289 & 27       \\   %\comments{& 76    &  -}\\
      \mockito{}         & 22,787  & 8,835 & 38    \\     %\comments{& 118   &  -}\\
      \bottomrule
  \end{tabular}
\caption {Characterization of \dfj{} subjects.}

\label{tab:df4j}
\end{table}
\normalsize

\begin{figure}[h]
	\vspace{-1.5cm}

		\centering
		\includegraphics[width=0.42\textwidth]{figures/defects4j.pdf}
		\vspace{-1cm}

		\caption{FAULT$\_$OF$\_$OMISSION distribution in the \dfj{} dataset}
		\label{fig:foos}
\end{figure}

Figure~\ref{fig:foos} shows the predominance of Fault Of Omission (FOO) cases in the dataset, i.e., 
faults which generally are difficult to locate. $59.7\%$ of the faults of this dataset are FOOs.


\subsection{Experimental Methodology}\label{sec:methodology}

To evaluate the effectiveness of \comb{}, following related work, we compared its
performance with \sfl{} for each fault from the dataset, for $k\in\{5,10\}$, 
where $k$ is the number of classes comprising the highest faulty statements. These 
classes are obtained directly from the ranking which is calculated through \sfl{}. 
For each fault, spectra is updated and the fault ranking is re-calculated but only 
considering the $k$ highest faulty classes. We refer to these variations as SFL$^{k}$. 
After slicing each class of the top $k$ of the highest faulty classes for each fault, 
the initial spectra is updated based on the slicer output and the fault ranking is 
re-calculated. We refer to these variations as \combpar{k}. We
compared \combpar{5} and \combpar{10} against \sfl{}$^{5}$ and \sfl{}$^{10}$ using the
following metric:

\begin{equation}
    \Delta{}C = C(\textrm{SFL$^{k}$}) - C(\textrm{\combpar{k}})
\end{equation}

\noindent
where $C$ is the cost of diagnosis. $C$ is the mean ranking of all buggy statements.
A positive $\Delta$C means that the position of the faulty statement has decreased
in the ranking after applying the \combpar{k} approach. Hence, reducing the cost of
locating the bug.

\subsection{Dynamic Slicing}\label{sec:ds}

This subsection aims to report the performance of the \ds{} technique on slicing de 
faults of $5$ \dfj{} projects. Table \ref{tab:red}
presents the reduction percentage obtained for each project after slicing the top 
$5$ and $10$ of classes comprising the highest faulty statements. Our approach was 
capable to reduce the \dfj{} projects in an average of $30.23\%$.

\begin{table}[h]
	\centering
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{lcc}
		\toprule
		Project             &  \combpar{5}  & \combpar{10} \\
		\midrule

        \lang{}            & 1.40\% & 31.46\%\\
        \cmath{}           & 10.30\% & 12.34\%\\
        %\closure{}          &  & \\
		\chart{}			& 59.30\% & 53.64\% \\
        \jtime{}            & 17.27\% & 32.02\%\\
        \mockito{}          & 16.54\% & 21.67\%\\

		\bottomrule
	\end{tabular}
	\caption {\ds{} reduction performance when considering $k=5$ and $k=10$}
	\label{tab:red}
\end{table}
\normalsize

\subsection{Results and Discussion}

Our approach was capable of slicing a total of $260$ faults from $5$ different 
sujects. We are still running experiments for the \closure{} and \cmath{} projects.
(The results of these faults will be integrated on the camera-ready if accepted).
$33$ of the $260$ faults do not include the faulty statements in the slicer report
after slicing.

\subsubsection{RQ1: \textit{How often DS misses faulty statements?}}
\label{sec:fault-misses}

In \textbf{RQ1}, our concern is to evaluate how often \ds{} misses faulty statements, 
since it is one of the main issues of dynamic slicing techniques. Table \ref{table:fsws} 
presents the number of faults where at least one of the faulty statements appears 
in the final reports of both techniques, \sfl{}$^{k}$ and \combpar{k}. We only search
for faulty statements in the top $5$ ($k=5$) and $10$ ($k=10$) of the highest faulty 
classes of the projects. \combpar{k} finds the faulty statements faster than 
\sfl{}$^{k}$. Using 



\begin{table*}[h]
	\small
	% \setlength{\tabcolsep}{3pt}
	\centering
	  \begin{tabular}{l|cccc|cccc|c}
		\toprule
		\multirow{2}{*}{Project}            & \multicolumn{4}{c|}{\sfl{}$^{k}$}  & \multicolumn{4}{c|}{\combpar{k}} & \multirow{2}{*}{\# Faults} \\

		            & \# $k = 5$ & \% $k = 5$ & \# $k = 10$ & \% $k = 10$
					& \# $k = 5$ & \% $k = 5$ & \# $k = 10$ & \% $k = 10$ &  \\
		\midrule
		 \lang{}         &  55 & 84.6\%  & 55  & 84.6\%
		 				& 65 & 100\% & 63 & 96.9\% & 65      \\
		\cmath{}           & 85 & 81.7\%  & 89 & 85.6\%
						& 93 & 89.4\% & 99 & 95.2\% & 104  \\   %\comments{& 177   &  -}\\
		% \closure{}         & 62.41 \%  & 72.18 \%  & 9.77 \%     \\   %\comments{& 350   &  -}\\
		\chart{}          & 22 & 84.6\% & 24 & 92.3\%
						& 20 & 76.9\% & 22 & 84.6\% & 26    \\
		\jtime{}          & 21 & 77.8 \% & 22 & 81.5 \%
				& 22 & 81.5\% & 23 & 85.2\% & 27     \\
		 \mockito{}      & 24  & 63.2\% & 27 & 71.1\%
		 				& 27 & 71.1\% & 30 & 78.9\% & 38 \\\midrule
	 Total      & 207  & 78.4\% & 217 & 83.0\%
	 				& 227 & 83.8\% & 237 & 88.2\% & 260 \\
		\bottomrule
	\end{tabular}
  \caption {Number of faults where at least one of the buggy-lines is in the \sfl{}$^{k}$ and \combpar{k} report}
  \label{table:fsws}
\end{table*}

\subsubsection{RQ2: \textit{How effective is the \comb{} combination for bug localization?}}


In \textbf{RQ2}, we intend to evaluate the impact of combining \ds{} with \sfl{} to improve \ds{}.

The \comb{} combination yields an average improvement of, approximately, 19\% on the ranking of the faulty statements of the \chart{}
project. Table , reports the average rank of all faulty statements for each version of \chart{} project after
applying the 3 techiniques \sfl{}, \comb{}-5 and \comb{}-10.

Table \ref{fig:diagnosis} reports the performance of the diagnosis using \comb{}-5 and \comb{}-10 instead of only using \sfl{}. $\Delta C <0$ means that the faulty statement may be removed. $\Delta C=0$ means that the faulty statement remained in the same position. $\Delta C >0$ means that \ds{} improved diagnosability.

%For both \comb{}-5 and \comb{}-10, diagnosability increases 55.56\%. However, top-5 has less classes to minimize than top-10.

Table \ref{table:st} presents the statistics to determine if the observed results are statistically significant. Shapiro-Wilk ~\cite{10.2307/2333709} tests the null hypothesis that results are drawn from a normal distribution. The test is perfomed for the three techniques under evaluation (\sfl{}, \comb{}-5, \comb{}-10). With 99 \% of confidence, the results tell us that the distributions are not normal. Given that the distributions are not normally distributed, we use the non-parametrical statistical hypothesis test Friedman ~\cite{10.2307/2279372}. The null-hypothesis is that all distributions are the same. With 99 \% of confidence, the results show that the distributions are not all the same. In order to understand if there was a pair of distributions that are equal, we performed a Nemenyi post-hoc analysis \Fix{cite}. Figure \ref{fig:performance} report the results of that analysis for all the distributions. With 95\% of confidence, it is possible to see that \sfl{} and \comb{}-5 have similar distributions.


% \begin{table}[h]
% 	\centering
% 	\setlength{\tabcolsep}{4pt}
% 	\begin{tabular}{ccc}
% 		\toprule
% 		$\Delta$$C$             &  \combpar{5}  & \combpar{10} \\
% 		\midrule
% 		$<0$  & 19.53\% & 16.02\% \\
% 		$=0$  & 46.88\% & 41.80\% \\
% 		$>0$  & 33.59\% & 42.19\% \\
%
% 		\bottomrule
% 	\end{tabular}
% 	\caption {Diagnosis Performance for using \combpar{5} and \combpar{10} instead of \sfl{}}
% 	\label{tab:diagnosis}
% \end{table}

\begin{figure}[h]
		\centering
		\includegraphics[width=0.42\textwidth]{figures/performance.pdf}
		\caption{Diagnosis Performance for using \combpar{5} and \combpar{10} instead of \sfl{}$^{5}$ and \sfl{}$^{10}$ respectively}
		\label{fig:diagnosis}
\end{figure}

\begin{table*}[h]
	\centering
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{@{}l|l|l|l|l@{}}
\toprule
  & SFL$^{5}$      & SFL$^{10}$             & \combpar{5}                 & \combpar{10}                 \\ \midrule
Mean & 584.33 (580.03)  &  516.42 (502.60) & 679.67 (566.99) & 613.98 (491.47)  \\ \midrule
Median & 49.38 (47.5)   & 53.00 (54.5)  & 69.25 (42.72)             & 56.93 (40)\\ \midrule
Variance & 1223.08 (1236.94)  &  1148.11 (1149.09)  & 1352.37 (1229.4) & 1294.87  (1143.02) \\ \midrule
Shapiro &\makecell{W = 0.53 \\ p = 7.13 x $10^{-26}$} & \makecell{W = 0.49 \\ p = 1.22 x $10^{-26}$} & \makecell{W = 0.55 \\ p = 2.64 x $10^{-25}$} & \makecell{W = 0.52 \\ p = 5.07 x $10^{-26}$}  \\ \midrule
Friedman & \multicolumn{4}{c}{\makecell{$\chi^{2}$ = 69.51 \\ p-value = 5.44 x $10^{-15}$}} \\
\bottomrule
\end{tabular}
  \caption {Statistical tests}
  \label{table:st}
\end{table*}

\begin{figure}[h]
	\vspace{-1.5cm}
		\includegraphics[width=0.45\textwidth]{figures/heatmap_nemenyi_result.pdf}
		\caption{Nemenyi post-hoc analysis results}
		\label{fig:performance}
\end{figure}


\subsection{Threats to validity}
%
A potential threat to external validity related to the set of programs used in
our study. When choosing the projects for our study, our aim was to opt for
projects that resemble a general and large-sized application. To reduce
selection bias and facilitate the comparison of our results, we decided to
choose a benchmark that is popular in the community~\cite{just-defects4j-issta2014}.

A potential threat to construct validity relates to the choice of oracle generation
for the Lithium toolset. However, we argue that our choice is able to capture whether
a program is failing because of the same reason.

The main threat to internal validity lies in the complexity of several of the tools
used in our experiments, most notably the Lithium toolset, the SFL diagnosis tool,
and the implementation of \comb{}.
%
\section{Related Work}
\Rui{needs work...}

\Mar{moved from intro$\rightarrow$} It is worth noting that Automated
Program Repair techniques should directly benefit from these
results---either to improve their results or to be aware they should
not invest on this integration.

\subsection{Program Slicing}

Program Slicing is an old technique with several applications in PL
and SE research~\cite{Weiser:1981:PS:800078.802557}. Slicing can be
implemented in different ways. Dynamic Slicing has found its main
application in fault
localization~\cite{Agrawal:1990:DPS:93542.93576}---in this application
context, failing tests exist. Research in this area has mainly focused
on slicing code
efficiently~\cite{Wang:2008:DSJ:1330017.1330021,Wang:2004:UCB:998675.999455}
and avoiding data and control omission
errors~\cite{Zhang:2007:TLE:1250734.1250782,Lin:2018:BDE:3238147.3238163}. Omission errors correspond to the
cases where tests fail because some part of the code was not executed
when it should. Dynamic fault localization techniques cannot hope to
soundly report bugs in those cases. The issue is that incorporating
static information to capture those cases can result in unacceptably
large slices, which defeats the purpose of reducing the fault search
space. It is worth noting that, conceptually, the \sfl{}+\ds{} combination
can be used with any form of dynamic slicing. We chose an
implementation of Critical
Slicing~\cite{DeMillo:1996:CSS:229000.226310} for its generality.



\subsection{SFL}

Statistics-based techniques (e.g., ~\cite{Pearson:2017:EIF:3097368.3097441}) are
popular automated fault localization techniques within the Software Engineering
community. They correlate information about program fragments that have been
exercised in multiple program execution traces (also called \textit{program
spectra}) with information about successful and failing executions. By doing
that, statistics-based approaches yield a list of suspect program fragments
sorted by their likelihood to be at fault. Since this technique is efficient in
practice, it is attractive for large modern software systems ~\cite{Zoeteweij:2007:DES:1251988.1253298}.

Spectrum-based fault localization (\sfl{}) is amongst the most common statistical
fault localization technique that takes as input a test suite including at least
one failing test and reports on output a ranked list of components likely to be
in fault ~\cite{FLSurvey2016,DBLP:conf/kbse/JonesH05,DBLP:journals/smr/LuciaLJTB14,DBLP:journals/jss/AbreuZGG09}.

\subsection{Combination of \sfl{} and Slicing}

Prior work has investigated the combination of dynamic slicing and
spectrum-based fault
localization ~\cite{Wotawa:2010:FLB:1848650.1849235,Alves:2011:FUD:2190078.2190115,DBLP:conf/ecai/HoferW12,lei-mao-dai-wang-2012,slicing-sfl-repair}. Although
the methodology used in these papers vary, the overall message is that
the combination is valuable. Note that slicing alone does not provide
the guarantee of improvement to \sfl{} as statements discarded with
slicing could have been ranked lower compared to faulty
statements. This paper differs from prior work in that it uses a much
larger dataset of programs and faults and a more rigorous experimental
methodology.


\section{Conclusions and Future Work}
\label{sec:conc}
%
Several approaches have been proposed in the literature to improve debugging
costs through automated software fault diagnosis to reduce time-to-market,
while maintaining high quality standards. In this domain, Dynamic Slicing (DS)
and Spectrum-based Fault Localization (SFL) are very popular fault diagnosis t
echniques and normally seen as complementary. This paper reports the outcome of
a study using real-world applications with the goal of demystifying the impact
of combining DS with SFL. The results of our empirical study show that, in
practice, DS rarely misses faulty statements (23 misses in 260 cases) and that
the DS-SFL combination improves the diagnostic accuracy in 51\% of the faults.
Hence, we found that, despite tacit opinions of the research community about
the usefulness of DS in automated debugging, we found the combination practical
and effective, and encourage new SFL techniques to be evaluated against that
optimization.

Future work is as follows: bla bla.

%
%\section*{Acknowledgments}
%include only after review.

{
  \small
  \balance
  \bibliographystyle{named}
  \bibliography{paper}
}

\end{document}
