\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai19}
% The file ijcai18.sty is the style file for IJCAI-18 (same as ijcai08.sty).

\input{packages}
\input{macros}

% the following package is optional:
\usepackage{latexsym}

\title{Demystifying the Combination of Dynamic Slicing and \\ Spectrum-based Fault
Localization}

% Single author syntax
\author{
    Submission 6364
}
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\hypersetup{draft}
\begin{document}

\maketitle

\begin{abstract}
Several approaches have been proposed to reduce debugging costs through
automated software fault diagnosis.  Dynamic Slicing (\ds{}) and Spectrum-based
Fault Localization (\sfl{}) are popular fault diagnosis techniques and normally
seen as complementary. This paper reports on a comprehensive
study\comments{--using \numFaults{} documented software faults from a credible
dataset--} to reassess the effects of combining \ds{} with \sfl{}.  With this
combination, components that are often involved in failing but seldom in passing
test runs could be located and their suspiciousness reduced. Results show that
the \ds{}-\sfl{} combination, coined as \comb, improves the diagnostic
accuracy up to $73.7\%$ (\avgImprov\ on average). Furthermore, results indicate
that the risk of missing faulty statements, which is a \ds{}'s key limitation,
is not high~---~\ds{} misses faulty statements in $9\%$\comments{($23$ misses)}
of the $\numFaults{}$ cases. To sum up, we found that the \ds{}-\sfl{} combination
was practical and effective and encourage new \sfl{} techniques to be evaluated
against that optimization.
\end{abstract}

\section{Introduction}

%% provide context (debugging + fault localization)
Software debugging is important and challenging. The task of locating
the faulty code (\ie{}, fault localization) is particularly
challenging. As such, countless automated techniques have been
proposed in the past to reduce the cost of fault
localization~\cite{7390282}. Model-based software diagnosis
(MBSD)~\cite{REITER198757,DEKLEER200325} and Spectrum-Based Fault
Localization (\sfl{})~\cite{DBLP:journals/stvr/HarroldRSWY00} are two
popular techniques that leverage different principles to automate
fault isolation~\cite{DBLP:conf/sac/AbreuGZG08}.  At a high-level,
MBSD
techniques~\cite{wotawa2002model,Mayer:2008:EMM:1642931.1642950,mayer2008prioritising,Perez:2018:LQR:3304889.3304927,Ko:2008:DRA:1368088.1368130}
attempt to eliminate non-suspicious components whereas \sfl{}
techniques attempt to rank suspicious components with the goal of
reducing fault diagnosis cost.

%% detail context (DS + SFL)
Dynamic Slicing (DS)~\cite{Agrawal:1990:DPS:93542.93576} is an
instance of MBSD that has attracted huge attention in research over
the last decades~\cite{Silva:2012:VPS:2187671.2187674}. The technique
traces back the statements in the code that influence a given point of
interest, such as the evaluation of a failing assertion.  Similarly to
\ds{}, \sfl{}~\cite{DBLP:journals/stvr/HarroldRSWY00} received
tremendous attention in research over the years~\cite{7390282}.  It
computes suspiciousness values associated with program components
(e.g., statements) based on coverage information gathered during the
execution of test cases.  More precisely, \sfl{} uses coverage
information of passing and failing test cases to identify likely
faulty statements and produces on output a list of program components
ranked in decreasing order of suspiciousness.

%% \ds{} and \sfl{} are complementary~\cite{DBLP:conf/sac/AbreuGZG08} and
%% could be combined.

Intuitively, \ds{} identifies irrelevant parts of the code (\ie{},
parts that do not contribute to the fault) whereas \sfl{} ranks the
relevant parts of the code. It is conceivable, therefore, to combine
these two techniques based on the intuition that highly ranked
statements (as per \sfl{}), albeit covered by failing executions,
could be in fact unrelated to the fault (as per \ds{}). In fact, prior
work reported promising preliminary results on this
combination~\cite{Wotawa:2010:FLB:1848650.1849235,Alves:2011:FUD:2190078.2190115,DBLP:conf/ecai/HoferW12,lei-mao-dai-wang-2012,slicing-sfl-repair}.
Unfortunately, they used a small set of subjects in their evaluation
or over optimistic methods to evaluate fault localization
improvement~\cite{Wu:2014:CLC:2610384.2610386,Lucia:2014:FFL:2642937.2642983,Wen:2016:LLB:2970276.2970359}.

Given the importance of fault localization, this paper revisits the
problem of assessing the effectiveness of combining \ds{} and \sfl{},
addressing the key issues of prior work. We conducted a comprehensive
study involving \numFaults{} faults from \numPrograms{} different
programs from the Defects4J
benchmark~\cite{just-defects4j-issta2014}, which is frequently used to
evaluate fault localization research. Results show that \ds{} misses
faulty statements in $9\%$\comments{(23 misses)} of the \numFaults{}
faults analyzed. Furthermore, we found that the combination of the two
approaches improves fault localization up to $73.7\%$
(\avgImprov\ on average). To sum up, results indicate that the
risk of applying the technique is relatively low for the positive
impact it may bring; and, the tool implementing the technique works
out-of-the-box, \ie{}, it puts no requirements on the running
environment, subject programs it can be used, and requires no special
setup.
% \Mar{do we have supporting data to say that
%   the improvement is bigger when we care the most? for the cases SFL
%   performs worse? if yes, can we show average for that?}

The contributions of this work are:
\begin{itemize}
 \item[1)]~an empirical study, using
real-world applications and bugs, on the combination of \ds{} and
\sfl{} for bug localization of \emph{Java} faulty programs and
\item[2)]~a
tool, dubbed \comb{}, implementing the \ds{}-\sfl{} combination.
\end{itemize}
The
tool as well as a replication package will be available once
double-blind requirements are lifted.

%% -----------------
%% This paper reports the results of a comprehensive study to assess the
%% impact of combining two complementary techniques to improve software
%% fault localization.

%% It is worth noting that

%%  We
%% developed a tool, coined as \comb{}, materializing the
%% combination.

%% the research community manifests
%% explicit~\cite{ang-perez-van-deursen-rui-2017,Pearson:2017:EIF:3097368.3097441,Xie:2016:RAD:2884781.2884834}
%% or tacit negative opinions about the usefulness of \ds{} and \sfl{} in
%% automated debugging.
%% -----------------

%Dynamic Slicing~\cite{Agrawal:1990:DPS:93542.93576}------and \sfl{}.
%%% explain what each technique does (at a high level)

%% Despite the skepticism of the research community, \sfl{} has been
%% shown useful in supporting downstream analyses, such as Automated
%% Program Repair
%% (\apr{})~\cite{automatic-software-repair-survey2017,kim-etal-daghstul2017},
%% an increasingly popular technique that looks for fixes to buggy
%% statements. Tools like JAFF \cite{arcuri-2011}, Prophet
%% \cite{long-rinard-2016}, SemFix \cite{nguyen-qi-roychoudhury-2013},
%% and SPR \cite{long-rinard-2015} use \sfl{} to guide the search for
%% likely fixes.

%% We found surprising that, despite these findings, no tool or client analyses use this combination
%% today.\Sof{@todo: Revise this and look to the paper from 2018} Several reasons could justify that observation. One hypothesis is that results reported in prior work are
%% over-optimistic. For example, most prior work evaluated improvements
%% of \sfl{} techniques using relative metrics, which are based on the
%% position of the first faulty statements found in the ranking relative
%% to the total number of ranked statements, which is often a large
%% number. As such, it inflates actual improvements and deceives
%% potential adopters of the technology. Ang \etal~\cite{ang-perez-van-deursen-rui-2017} recently
%% pointed to that fact and encouraged researchers to adopt more precise metrics,
%% such as \topk{}
%% \cite{Wu:2014:CLC:2610384.2610386,Lucia:2014:FFL:2642937.2642983,Wen:2016:LLB:2970276.2970359},
%% which has been widely adopted to evaluate performance of information retrieval algorithms~\Fix{cite}. This
%% metric reports the percentage of faults captured by a technique when
%% the rank is trimmed to the first $k$ components.

%\section{Background}
%This section summarizes the ground concepts of our approach.

\section{Dynamic Slicing (\ds{})}
\label{sec:slicing}


Model-based diagnosis is a well-known approach that has been proposed by the
DX community, a sub-field of AI that develops algorithms and techniques to
determine the root-cause of observed failures.~\cite{REITER198757,DEKLEER200325}. Applications
of model-based diagnosis to localize software faults have demonstrated that it
can be framed as dynamic slicing~\cite{Mayer:2008:EMM:1642931.1642950,mayer2008prioritising}.

Dynamic slicing~\cite{Agrawal:1990:DPS:93542.93576}\comments{~--~as opposed to
static slicing~\cite{Weiser:1981:PS:800078.802557}~--~~\cite{Binkley:2014:OLP:2635868.2635893}}
is an instance of model-based software diagnosis that
has been shown useful in automated software debugging where the region
of interest is restricted to what can be reached from failing
tests. Several dynamic slicing techniques exist. This paper uses
Critical Slicing (\cs{})~\cite{DeMillo:1996:CSS:229000.226310} for its
simplicity/generality. Critical Slicing
prescribes a black-box language-semantics-agnostic recipe to computing
executable slices. Critical Slicing simplifies the original
program such that the resulting program preserves critical
observations, such as assertion violations. More precisely, the
simplification mechanism consists of deleting statements on the
original program and checking if the output of the original and
modified program are the same.

Our implementation of Critical Slicing is based on the Mozilla Lithium
tool\footnote{Lithium details available at {\footnotesize\url{https://github.com/MozillaSecurity/lithium}} (Accessed \today)}. It takes as input a file
and produces as output a simplified version of that file that
satisfies a user-defined oracle. In our case, the oracle is defined
such that the test produces the same failure manifestation as the one
observed with the test execution on the original program. The
Lithium minimization process starts by determining the initial size---in
number of lines---of chunks to delete from the input file. For that,
it chooses the highest power of two number smaller than the file
size. For example, if the file has $1,000$ lines, Lithium sets the
initial chunk size to $512$ lines. Then, the tool starts a local search looking
for chunks to exclude from the file. If the chunk satisfies the oracle it is removed.
When no more chunks of that given size can be removed, Lithium
divides the chunk size by two and repeats the search. This iterative
process continues until no more lines can be removed.  If $n$ is the
size of the input file and $m$ is the size of the 1-minimal file found
by Lithium, then Lithium usually performs $O(m\cdot\lg(n))$
iterations. Proofs of the algorithm complexity can be found
elsewhere\footnote{Lithium complexity available at \url{https://github.com/MozillaSecurity/lithium/blob/master/src/lithium/docs/algorithm.md} (Accessed \today)}. Our implementation is publicly
available at \textit{$<$anonymized for double-blind review$>$}.

\begin{theorem}\label{the:1}
  The faulty statement may not be included in the critical slice of
  failing test cases.
\end{theorem}

If test oracles are too general, it is possible, conceptually, that
the critical slicing algorithm produces slices without the faulty
code. Suppose that a test fails with a Null-Pointer Exception (NPE)
and the criterion used to slice the code in that case (\ie{}, the
oracle) is the presence of NPE in the output, regardless of the
location that raises that exception. It is therefore possible that the
slice obtained at some iteration of the aforementioned algorithm
raises NPE, but it does so in a different part of the code. If that
happens, the critical slicing algorithm would consider that as an
acceptable simplification--as it satisfied the criterion--and would
continue. As the algorithm does not backtrack, it would be impossible,
to obtain a slice containing the faulty component from that point
on.\hfill{\tiny$\blacksquare$}

It is worth noting that this is not a problem that afflicts only CS;
\emph{all} purely dynamic slicing techniques manifest this
problem~\cite{Lin:2018:BDE:3238147.3238163}. To mitigate the issue in
CS, it is necessary to make oracles more specific. For that, our
approach was to use as the slicing criterion the stack trace
associated with the test failure. That decision increases the chances
that the sliced program will follow a path to the error similar to
that followed by the original program.
In addition, there are faults whose outputs
may slightly differ from one execution to another. For example,
exceptions including memory address identifiers, which change on
each execution. For these cases, it was developed a mechanism to compare the
rest of the output ignoring the identifier of the memory address.

% \subsection{Implementation}
% \label{sec:impl}

% \Mar{Sofia, I think this was way too low-level for intro and decided
%   to move here. please check how to fit.}
% \Fix{
% A couple of different tools were designed to perform this empirical study: \morpho{} and \lithium{}. \morpho{} retrieves as an output the input for \lithium{}. \lithium{} uses the input to reduce the search domain for \sfl{} and outputs the statmeents that resulted from the minimization. \morpho{} uses this to update the spectrum matrix, performs the before and after \sfl{} evaluation using the corresponding matrix, and outputs the metrics that report the \sfl{} performance for both cases ~---~ before and after using \ds{}. }

% Two different tools were developed to support this research: \morpho{} and \lithium{}. \morpho{} was designed to calculate the suspiciousness of all statements of a project before, and after the top-k minimization performed by \lithium{} whereas \lithium{} is responsible for reducing the search domain of each \texttt{Java} class of the project for the after-\sfl{} analysis. \morpho{} uses the spectrum matrix (Figure \ref{fig:spectrum-example}), and the pair \emph{name\#location} for each statement to calculate the respective ranking. All rankings are ordered from highest to lowest ranked. This information is retrieved along with each test case stack trace in a \texttt{.json} file which serves as an output to \lithium{}. \lithium{} starts by generating the inputs (Algorithm \ref{alg:ls}, line $2$) for the top-k classes of each failing test based, mainly, on the output of \emph{morpho} ~---~ ranked list of statements and the test cases stacktraces. Then, the tool iterates each class from the top-k classes ($c$) of each test $t$. In each iteration, classes are refined using an external java program (Algorithm \ref{alg:ls}, line 6) that substitutes all the line comments ($\backslash\backslash$), block comments ($\backslash*$ to $*\backslash$) and javadoc comments ($/**$ to $*/$) using the \texttt{JavaParser}\footnote{JavaParser is available at http://javaparser.org/ (accessed November 2018).} library. This step was added to \lithium{} because it turns \texttt{MozillaLithium} faster since the empty lines are ignored. Then, \texttt{MozillaLithium} performs the class minimization using a function of interest (Algorithm \ref{alg:finc}) which compares the output of the test with the expected one which is given as an input. Finally, the location of all relevant statements (Algorithm \ref{alg:ls}, line 9) is saved in a \texttt{.json} which is used for the before-\sfl{} analysis. \morpho{} uses the output from \lithium{} to create a copy of the older spectrum matrix and updates it according to the explanation provided on Figure \ref{fig:ds-reduction} where the statements that are not in the slice of the test suffer a suspiciouness reduction. In the end, \morpho{} performs the \sfl{} analysis for both matrixes and calculates the probability of the first line being faulty, the probability of the last line being faulty, and the mean and median of the position of the faulty line in the ranking. These are the metrics used to evaluate how considerable is the improvement obtained when combining \ds{} with \sfl{}.

% \begin{algorithm}[h]
% 	\caption{Class Minimization Algorithm}
% 	\label{alg:ls}
% 	\begin{flushleft}
% 		\textbf{Input:} $proj$ - project name \\
% 		\hspace{2.75em} $bug$ - bug number\\
% 		\hspace{2.75em} $k$ - number of top ranked classes\\
% 		\hspace{2.75em} $stk$ - expected stacktrace\\
% 	 \textbf{Output:} Top-k classes minimization for each test \\
% 	\end{flushleft}
% 	\begin{algorithmic}[1]
% 		\Procedure{lithium-slicer}{$proj$, $bug$, $k$, $stk$}
% 			\State $testsInfo \leftarrow$ generateInputs($proj$, $bug$, $k$, $stk$)
% 			\ForAll {$t \in testsInfo$}
% 				\State $classes \leftarrow$ getClasses($proj$, $bug$)
% 				\ForAll {$c \in classes$}
% 					\State $unc \leftarrow$ removeComments($c$)
% 					\State $min \leftarrow$ MozillaLithium($iFunc$, $unc$, $t$, $stk$)
% 				\EndFor
% 				\State $slicer \leftarrow$ getLocation($c$, $t$, $min$)
% 			\EndFor
% 			\State \Return $slicer$
% 		\EndProcedure
% 	\end{algorithmic}
%
% \end{algorithm}
%
% \begin{figure*}[t]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{figures/lithium.pdf}
% 	\caption{Simple illustration of the process to obtain the top-5 minimized classes from a project carrying a bug}
% \end{figure*}
%

%---------------------------------------------------------
%% \subsubsection{\label{sec:oracleheuristics}Oracle Heuristics.}~


%% The \cs{} technique is somewhat based on the \textit{statement deletion} mutant
%% operator of the mutation-based testing methodology. This operator was designed
%% based on the idea that each statement has an effect on the program output (i.e.,
%% oracle). Thus, testers are encouraged to design test sets that cause all
%% statements to be executed and generate outputs that are different from the
%% program under test.

%% Mozilla's Lithium takes as input a function of interest that can be designed
%% according to the problem to be solved. The function of interest determines if
%% the test case output is interesting or not. Our function simply compares the
%% failing test output with the output obtained when re-running again the failing
%% test after deleting the slice, as explained in Section~\ref{sec:slicing}. Thus,
%% if the initial test output is the same as the one after removing the slice
%% \{$c_2$,$c_5$\}, then the slice is removed. This means that the slice is not
%% necessary to reveal the fault because if removed the test output is still the
%% same. The output is extracted from the stacktrace obtained through the failing
%% test execution. The stacktrace provides information not only about the place
%% where the fault was revelead but also the path taken to get there. Thus, our
%% approach considers not only the exception caught by the failing test but also
%% the path that was taken to reveal the fault. The following provides an example:

%% \vspace{3mm}
%% \begin{myframe} \label{box:1}
%% junit.framework.AssertionFailedError, \\
%% at junit.framework.Assert.fail(Assert.java:55), \\
%% at junit.framework.Assert.assertTrue(Assert.java:22), \\
%% at junit.framework.Assert.assertFalse(Assert.java:39), \\
%% at junit.framework.Assert.assertFalse(Assert.java:47), \\
%% at junit.framework.TestCase.assertFalse(TestCase.java:219), \\
%% at org.jfree.chart.util.junit.ShapeUtilitiesTests.\\testEqualGeneralPaths(ShapeUtilitiesTests.java:212), \\
%% ...
%% \end{myframe}

%% Considering some of the stacktrace information about the path taken to reveal
%% the fault solved the issue of generic oracles (e.g., NPE or generic
%% AssertionFailedError expections as the one presented before) mentioned in
%% Theorem~\ref{the:1}. There are faults whose outputs could slightly differ from
%% one execution to another. For example, exceptions including information of
%% memory addresses. For these cases, it was developed a mechanism to compare the
%% rest of the output ignoring the identifier of the memory address. This heuristic
%% also handles StackOverflow oracles.
%---------------------------------------------------------

\section{Spectrum-based Fault Localization (\sfl)}
\label{sec:sfl}

\begin{wrapfigure}[8]{1}{0.25\textwidth}
  \hspace{-2ex}
  \centering
  \scriptsize
  \setlength{\tabcolsep}{3.5pt}
  \begin{tabular}{c|cccc|c}
    $\mathcal{T}$ & $c_1$    & $c_2$    & $\cdots$ & $c_M$    & $e$    \\ \hline
    $t_1$         & $\A{11}$ & $\A{12}$ & $\cdots$ & $\A{1M}$ & $e_1$  \\
    $t_2$         & $\A{21}$ & $\A{22}$ & $\cdots$ & $\A{2M}$ & $e_2$  \\
    \vdots        & \vdots   & \vdots   & $\ddots$ & \vdots   & \vdots \\
    $t_N$         & $\A{N1}$ & $\A{N2}$ & $\cdots$ & $\A{NM}$ & $e_N$  \\
  \end{tabular}
  \caption{An example spectrum.}
  \label{fig:spectrum-example}
\end{wrapfigure}

Spectrum-based fault localization is a statistical fault
localization technique that takes as input a test suite including at
least one failing test and reports on output a ranked list of
components likely to be in
fault~\cite{7390282,DBLP:conf/kbse/JonesH05,DBLP:journals/smr/LuciaLJTB14,DBLP:journals/jss/AbreuZGG09}. The
following are given in \sfl{}: a finite set $\mathcal{C}=\set{c_1,c_2,...,c_M}$
of $M$ system \emph{components}\footnote{A
component can be any code artifact of arbitrary granularity
such as a class, a method, or a statement~\cite{DBLP:journals/stvr/HarroldRSWY00}.};
a finite set $\mathcal{T} = \set{t_1,t_2,...,t_N}$ of $N$ system transactions,
which correspond to records of a system execution, such as test cases;
the error vector $e = \set{e_1,e_2,...,e_N}$, where $e_i = 1$ if
transaction $t_i$ has failed and $e_i = 0$ otherwise; and an
$N\times{}M$ coverage matrix $\mathcal{A}$, where $\A{ij}$ denotes the
coverage of component $c_j$ in transaction $t_i$.  The pair
$(\mathcal{A},e)$ is commonly referred to as
spectrum~\cite{DBLP:journals/stvr/HarroldRSWY00}. Figure~\ref{fig:spectrum-example}
shows an example spectrum.

Several types of spectra exist. The most commonly used is called
hit-spectrum, where the coverage matrix is encoded in terms of binary
\emph{hit} (1) and \emph{not hit} (0) flags, \ie{}, $\A{ij} = 1$ if
$t_i$ covers $c_j$ and $\A{ij} = 0$ otherwise.  \sfl{} takes as input
the pair $(\mathcal{A},e)$ and produces on output a list of components
ranked by their faulty suspiciousness. To that end, the first step of
the technique consists of determining what columns of the matrix $A$
resemble the error vector $e$ the most.  For that, an intermediate
component frequency aggregator $n_{pq}(j)$ is computed $n_{pq}(j) =
|\{i\mid \A{ij}=p \wedge e_i=q\}|$. $n_{pq}(j)$ denotes the number of
runs in which the component $j$ has been active during execution ($p =
1$) or not ($p=0$), and in which the runs failed ($q = 1$) or passed
($q = 0$).  For instance, $n_{11}(j)$ counts the number of times
component $j$ has been involved ($p = 1$) in failing executions ($q =
1$), whereas $n_{10}(j)$ counts the number of times component $j$ has
been involved in passing executions. We then calculate similarity to
the error vector by means of applying \emph{fault predictors} to each
component to produce a score quantifying how likely it is to be
faulty.  Components are then ranked according to such likelihood
scores and reported to the user. Ochiai is one of those fault
predictors that has shown to perform
well~\cite{7390282,Pearson:2017:EIF:3097368.3097441}. The Ochiai
formula is given by the following equation
$\textit{ochiai}\,$=$\,\N{11}/\sqrt{(\N{11}+\N{01}) * (\N{11}+\N{10})}$.

\section{The \comb{} Approach}
\label{sec:approach}
\label{sec:comb}

This section describes \combpar{}, a technique where \ds{} and \sfl{}
work in tandem to solve the fault localization problem. To illustrate
the idea behind \combpar{}, consider a debugging scenario with five
components $c_{1..5}$ and five transactions $t_{1..5}$, two of which
are failing. Consider, additionally, that the component $c_2$ is
faulty. Figure~\ref{fig:illustration} shows, at the left-hand side of
the arrow ($\Rightarrow$), an hypothetical spectra and its
corresponding ranking, produced with the Ochiai predictor. Each line
in the ranking shows, respectively, the rank/position, the component
label, and the Ochiai score (in parentheses).

We illustrate the workflow of the \comb{} approach for this scenario
in the following. Let us analyze components at the granularity of
statements. First, \comb{} picks the first two statements at the top
of the ranking, $c_4$ and $c_3$, for further analysis.  (The cutoff
point is user-defined.) Second, the technique picks a failing test to
slice the code, say $t_2$. A slice is a set of statements. Let us
assume that the slice obtained for the transaction $t_2$ is $\{c_2,
c_5\}$, \ie, it excludes statements $c_4$ and $c_3$ that were
previously covered by the test. Finally, the spectra and ranking are
updated. The right-side of Figure~\ref{fig:illustration} shows the
modified spectra and ranking after slicing the code against $t_2$. It
is worth noting that the technique slices the code for every failing
tests. Intuitively, slicing enables the identification of statements
in the spectra whose values can be set to zero, \ie{}, the components
marked with $0$ are irrelevant to determine the test output.

Let us now observe the effect of this spectra modification on the
ranking. Analyzing the Ochiai formula, one observes that, for the
components $j$, which are not part of the slice of a failing test, the
combination reduces the value of $\N{11}$ and increases the value of
$\N{01}$. Therefore, suspiciousness of those components
decrease. Similar argument applies to other fault predictors. In the
running example from Figure~\ref{fig:illustration}, the components
$c_3$ and $c_4$, which are not in the slice of $t_2$, have their
suspiciousness reduced, enabling the faulty component $c_2$ to rise
from the third to the first position in the ranking.

\vspace{1ex}
Briefly, the workflow of \comb{} consists of four steps:
\begin{enumerate}
  \itemsep0em
  \item\label{step:spectra} Compute spectra $S$ and ranking $R$ for
  the input test suite;
  \item\label{step:susp-files} Select top $k$ most suspicious classes, according to $R$;
  \item\label{step:compute-slice} Compute slicer for every suspicious
    file, obtained in Step~\ref{step:susp-files}, and every failing test;
  \item\label{step:group} Assemble all the resulting slicers of each failing test;
  \item Adjust spectra $S$, from step~\ref{step:spectra}, with
    the slices of each failing test, from
    step~\ref{step:group}, and then
	recompute the ranking.
\end{enumerate}

\begin{theorem}
  The rank of faulty statements cannot decrease if the slice includes
  the faulty statements.
\end{theorem}

The proof is based on the outlined argument that irrelevant
components $j$ have their ranks reduced as $\N{11}$ decreases and
$\N{01}$ increases. If those irrelevant components appear at positions
above the faulty component, it is possible that the faulty component
becomes more suspicious relative to $j$, \ie{}, the ranking of the
faulty component increases. If those components appear at positions
below the faulty components, the ranking of the faulty components is
unaffected.\hfill{\tiny$\blacksquare$}

\begin{figure}[t!]

  \centering
  \begin{subfigure}{0.5\textwidth}
    {\def\arraystretch{0.9}\setlength{\tabcolsep}{3pt}
      \begin{tabular}{c|ccccc|c}
        $\mathcal{T}$ & $c_1$    & $c_2$   & $c_3$ & $c_4$ &  $c_5$   & $e$    \\ \hline
        $t_1$         & 1 & 0 & 1 & 1 & 0 &\cmark  \\
        $t_2$         & 0 & 1 & 1 & 1 & 1 &\xmark  \\
        $t_3$         & 1 & 0 & 1 & 0 & 0 &\xmark  \\
        $t_4$         & 0 & 1 & 0 & 0 & 1 &\cmark  \\
        $t_5$         & 1 & 0 & 0 & 1 & 1 &\cmark \\
        \hline
      \end{tabular}
      \quad
      $\Rightarrow$
      \quad
      \begin{tabular}{c|ccccc|c}
        $\mathcal{T}$ & $c_1$    & $c_2$   & $c_3$ & $c_4$ &  $c_5$   & $e$    \\ \hline
        $t_1$         & 1 & 0 & 1 & 1 & 0 &\cmark  \\
        $t_2$         & 0 & 1 & {\cellcolor{Gray} 0} & {\cellcolor{Gray} 0} & 1 &\xmark  \\
        $t_3$         & 1 & 0 & 1 & 0 & 0 &\xmark  \\
        $t_4$         & 0 & 1 & 0 & 0 & 1 &\cmark  \\
        $t_5$         & 1 & 0 & 0 & 1 & 1 &\cmark  \\
        \hline
      \end{tabular}
    }
    \caption{Spectra update.}
    \label{fig:ds-reduction}
  \end{subfigure}

  \vspace{2ex}

  \begin{subfigure}{0.5\textwidth}
    \centering
    \begin{tabular}{cccccc}
      {\cellcolor{Gray}1} & {\cellcolor{Gray}$c_4$ (0.59)} & & 1 & $c_2$ (0.35) \\
      {\cellcolor{Gray}2} & {\cellcolor{Gray}$c_3$ (0.55)} & & 2.5 & $c_1$ (0.32)\\
      3 & $c_2$ (0.35) & \hspace{1ex}$\Rightarrow$\hspace{1ex} & 2.5 & $c_5$ (0.32)\\
      4.5 & $c_1$ (0.32) & & {\cellcolor{Gray}4} & {\cellcolor{Gray}$c_3$ (0.29)}\\
      4.5 & $c_5$ (0.32) & & {\cellcolor{Gray}5} & {\cellcolor{Gray}$c_4$ (0.00)}\\
    \end{tabular}
    \caption{Ranking update.}
  \end{subfigure}

  \caption{Modifications on spectra and ranking as result of slicing
    code against test $t_2$. Double arrow ($\Rightarrow$) indicates
    before and after.}
  \label{fig:illustration}
  \vspace{-2ex}
\end{figure}

%
\section{Evaluation}
\label{sec:eval}

We studied the following research questions:

%% The goal of this paper is to reassess whether the \comb{} approach yields
%% promising results under a more rigorous experimental methodology and a larger
%% dataset.\comments{The D4J dataset is about twice as large than the biggest test
%% suite used in previous studies.}

\newcommand{\rqone}{How effective is DS in eliminating code?}
\newcommand{\rqtwo}{How often does DS miss faulty statements?}
\newcommand{\rqthree}{How effective is \comb{} for bug localization?}

\begin{itemize}[leftmargin=0em]
\item[]{\textbf{RQ1.}}~\textit{\rqone{}}
\item[]{\textbf{RQ2.}}~\textit{\rqtwo{}}
\item[]{\textbf{RQ3.}}~\textit{\rqthree{}}
\end{itemize}

The first question evaluates the ability of dynamic slicing to reduce
the size of application code. Improvements on fault localization
depend on that ability. The second question addresses the problem that
affects all dynamic slicing
techniques~\cite{Lin:2018:BDE:3238147.3238163}---that of missing
faulty statements (see Theorem~\ref{the:1}). Developers would
not be able to successfully debug code for the cases that issue is
manifested. The third research question evaluates the impact of the
combination, as substantiated by \comb, to fault localization.

%%  shows that CS can
%% miss faulty statements because of imprecise oracles that guide the
%% slicing process.
%% Different slicers can miss faulty statements for different
%% reasons. In this paper, we used Critical
%% Slicing~\cite{DeMillo:1996:CSS:229000.226310}, which, in principle,

%
%
\subsection{Objects of Analysis}\label{sec:analysis}


We used subject programs from the \dfj{}
benchmark~\cite{just-defects4j-issta2014} in our evaluation.
Table~\ref{tab:df4j} shows size, number of tests, and number of
faults, for each considered program. Two faults from \cmath{} were not
considered because we were unable to reproduce them in our
environment.

\lang{} is a library providing utility functions for the
     {\small\texttt{java.lang}} API. \cmath{} is a library of
     self-contained mathematics and statistics components.\comments{
       The \closure{} is a toolset for turning JavaScript files into
       smaller scripts for faster download and execution in the
       browser.} \chart{} is a Java library for creating charts.
     \jtime{} is a library for manipulating date and time\comments{,
       providing a simpler API compared to
       {\small\texttt{java.util.Date}}}. \mockito{} is a mocking
     testing framework.

\begin{table}[h]
  \small
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{lrrr}
    \toprule
    Project            & Size (LOC) & \# Tests & \# Faults \\ %\comments{& Failing Test Cases &}
    \midrule
    \lang{}            & 111,751  & 6,057 & 65       \\   %\commentst{& 124   &  -}\\
    \cmath{}           & 306,276  & 26,797 & 104     \\   %\comments{& 177   &  -}\\
    %      \closure{}         & 149,521  & 27,930  & 133     \\   %\comments{& 350   &  -}\\
    \chart{}           & 230,159  & 8,458 & 26      \\  %\comments{& 92    &  -}\\
    \jtime{}           & 141,610  & 3,289 & 27       \\   %\comments{& 76    &  -}\\
    \mockito{}         & 22,787  & 8,835 & 38    \\     %\comments{& 118   &  -}\\
    \bottomrule
  \end{tabular}
  \caption {Characterization of \dfj{} subjects.}
  \label{tab:df4j}
\end{table}
\normalsize


%% Originally, this benchmark includes six different subjects and 395
%% faults (Table \ref{tab:df4j}).

%%%%%%%%%%%%%%%%%%%%% I commented because there are just two cases;
%%%%%%%%%%%%%%%%%%%%% not much information added (that could not be
%%%%%%%%%%%%%%%%%%%%% said in text). -Marcelo
%% \begin{figure}[h]
%%   \vspace{-0.5cm}
%% 		\centering
%% 		\includegraphics[width=0.42\textwidth]{figures/defects4j.pdf}
%% 		\vspace{-0.75cm}
%% 		\caption{Faults of omissions' distribution}
%% 		\label{fig:foos}
%% \end{figure}

%% a popular metric in fault
%% localization research for that
%% purpose.  A fault of omission
%% (\foo) occurs when the program is buggy because some code is missing,
%% i.e., a fix to the bug is created by adding code as opposed to
%% modifying or changing code.

As to quantify complexity of fault localization on this benchmark, we
measured the frequency of ``faults of omission'' (\foo), which is a
fault whose fix is materialized only with the addition of new
code~\cite{Pearson:2017:EIF:3097368.3097441}. Intuitively, the ability
of techniques to produce accurate diagnostic reports in those cases
might be poor as existing statements are not problematic. We found
that this dataset has a total of $59.7\%$ of \foo s, making it
challenging for techniques to perform well.

% \Mar{Sofia, does it make sense to show a histogram of (initial)
%   rankings of pure SFL to complement this argument? Say, frequency of
%   cases SFL reports on <top-10 statements in the ranking, then
%   frequency within top-10> and <top-20, etc.}

\subsection{Techniques}
One important independent variable for this study is the scope of
analysis for the techniques. Analyzing long rankings would be
unacceptably expensive not only for
humans~\cite{Parnin:2011:ADT:2001420.2001445} but also for
machines. For example, Critical Slicing needs to re-compile the
affected file at every iteration, \ie{}, after deleting statements and
before checking the oracle (see Section~\ref{sec:slicing}).
%% how we do it
We control the scope of analysis through the variable $k$, denoting
the number of most-suspicious files that will be analyzed. These files
are selected from the \sfl{} ranking as follows. First, we build a
ranking of files by determining, for each ranked statement, the file
that declares it. Then, we select the top-$k$ distinct files from such
file ranking.

The techniques we evaluate in this study are \combpar{k} and
SFL$^{k}$, which is the comparison baseline. \combpar{k} is as defined
in Section~\ref{sec:approach} (see Step $2$ of the workflow) whereas
SFL$^{k}$ is as SFL, but it produces a ranking only including the
statements from the top-$k$ ranked files. In this study, we used $5$
and $10$ as the values of $k$, following the same choice as in
previous fault localization
studies~\cite{ang-perez-van-deursen-rui-2017}.

%% To evaluate the effectiveness of \comb{}, following related work, we compared
%% its performance with \sfl{} for each fault from the dataset, for $k\in\{5,10\}$,
%% where $k$ is the number of classes comprising the highest faulty statements.
%% These classes are obtained directly from the ranking which is calculated through
%% \sfl{}.



%% For each fault, spectra is updated and the fault ranking is
%% re-calculated but only considering the $k$ highest faulty classes. We refer to
%% these variations as SFL$^{k}$. After slicing each class of the top $k$ of the
%% highest faulty classes for each fault, the initial spectra is updated based on
%% the slicer output and the fault ranking is re-calculated. We refer to these
%% variations as \combpar{k}.

\subsection{Results and Discussion}

This section discusses and answers the research questions.

%% proposed and discuss the
%% results obtained. Our approach was capable of slicing a total of $260$ faults
%% from $5$ different  projects.
%% \textbf{RQ1} evaluates how effective is \ds{} on reducing the test cases, \textbf{RQ2} reviews
%% one of the main issues of \ds{} - missing faulty statements - and
%% \textbf{RQ3} discusses how effective is the \combpar{k} approach against
%% \sfl{}$^{k}$.




%% This subsection aims to report the performance of the \ds{} technique
%% on slicing de faults of $5$ \dfj{} projects.


\subsubsection{\label{rq:1}RQ1: \textit{\rqone}}

A necessary but insufficient condition for improvement of fault
localization with \comb{} is that dynamic slicing eliminates a high
amount of code from the best ranked files. Table \ref{tab:red} shows,
as percentages, the size reduction of the top files obtained with
dynamic slicing for different values of $k$. Overall, results indicate
that dynamic slicing substantially reduces the size of highly-ranked
files.

%\Mar{+ coverage?}

%\Mar{sofia, it would be more convincing if you (additionally)
%  show reduction wrt to the the parts of the file that are covered.}

%% obtained for each project after slicing the top $5$ and
%% $10$ of classes comprising the highest faulty statements. We observed
%% a reduction of $30.23\%$ in the projects considered in our study.

\begin{table}[h]
  \small
	\centering
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{lrr}
		\toprule
		Project             &  \multicolumn{1}{c}{$k=5$} & \multicolumn{1}{c}{$k=10$} \\ %\combpar{5}  & \combpar{10}
		\midrule

        \lang{}            & 1.40 & 31.46\\
        \cmath{}           & 10.30 & 12.34\\
        %\closure{}          &  & \\
		\chart{}			& 59.30 & 53.64 \\
        \jtime{}            & 17.27 & 32.02\\
        \mockito{}          & 16.54 & 21.67\\

		\bottomrule
	\end{tabular}
	\caption {\ds{} reduction in file size (percentages). Higher is better.}
	\label{tab:red}
\end{table}
\normalsize

\subsubsection{RQ2: \textit{\rqtwo}}
\label{rq:2}


%% For automated debugging that
%% means that it will not be in the diagnostic report and therefore has a negative
%% impact in the fault localization accuracy.


%% In \textbf{RQ2}, our concern is to evaluate how often \ds{} misses faulty
%% statements, since it is one of the main issues of dynamic slicing techniques. This
%% is particularly important because when \ds{} misses a faulty statement

%% of both techniques, \sfl{}$^{k}$ and \combpar{k}

A fundamental issue of purely dynamic slicing techniques is the risk
of discarding faulty statements (see Section~\ref{sec:slicing}). This
research question evaluates the practical impact of this conceptual
issue. Table~\ref{tab:ds-cases-captured} shows the number of cases
dynamic slicing captures at least one of the faulty statements among
the sliced files for different values of $k$. These numbers indicate
that in $87.3\%$ of the cases, on average, dynamic slicing finds at
least one faulty statement in the top $5$ of the highest-ranked faulty
classes. This number is slightly improved to $91.2\%$ if we consider
the top $10$ of the highest-ranked faulty classes.

\begin{table}[h]
  \small
	\centering
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{lrr}
		\toprule
		Project             &  \multicolumn{1}{c}{$k=5$} & \multicolumn{1}{c}{$k=10$} \\ %\combpar{5}  & \combpar{10}
		\midrule

        \lang{}            & 100.0 & 96.9\\
        \cmath{}           & 89.4 & 95.2\\
        %\closure{}          &  & \\
		\chart{}			& 76.9 & 84.6 \\
        \jtime{}            & 81.5 & 85.2\\
        \mockito{}          & 71.1 & 78.9\\\midrule
        Total          & 87.3 & 91.2\\


		\bottomrule
	\end{tabular}
	\caption {Dynamic Slicing performance on capturing faulty statements, as
     percentages. Higher is better.}
   \label{tab:ds-cases-captured}
\vspace{-0.2cm}

\end{table}
%\normalsize

%% Table~\ref{table:fsws} presents the number of faults where at
%% least one of the faulty statements appears in the final
%% reports.\Mar{$\leftarrow$this seems a sensible choice that needs
%%   reasonable (to-the-point) justification. is there a rationale for
%%   this?  if not, was this choice used in other study? if yes, just say
%%   it was used and cite.}  More precisely, we look for faulty
%% statements in the top 5 and 10 most suspicious classes (as per the
%% file ranking).
%% \Mar{CRITICAL: Sofia, I am really confused here. Is there a reason for
%%   why you chose not to show the proportion of cases where **DS** (not
%%   SFL(k) or \combpar{k}) includes the faulty statement? Wouldn't it be
%%   sufficient and simpler/clearer answering this question in terms of
%%   DS for k=5,10? (Note that the question is about DS.) On a similar
%%   note, considering this metric, I don't understand how SFL(k) can
%%   produce different result compared to \combpar{k}. I thought the
%%   difference in this case would only be the position in the
%%   ranking. This is counter-intuitive and needs a good explanation.}

%% \Mar{my
%%   opinion is that we should not compare these two techniques here: 1)
%%   it is not the goal of this RQ and 2) this is not the best metric.}

To analyze the impact of dynamic slicing in context, we also compared
\combpar{k} and \sfl{}$^{k}$ considering the number of cases where the
fault is captured by the technique within a certain bound $k$. Note
that the reason \sfl{}$^{k}$ misses the fault is different compared to
\combpar{k}. \sfl{}$^{k}$ captures the fault if it is included within
the top $k$ files; it misses the fault
otherwise. Table~\ref{table:fsws} shows results, indicating that
\combpar{k}, typically, captures more faults compared to
\sfl{}$^{k}$. In only two cases, highlighted in gray color,
\sfl{}$^{k}$ outperformed \combpar{k}. These results show that the
majority of faulty statements are found on the $5$ highest ranked
faulty classes and that \combpar{10} is the technique
that performs better, missing only $23$ out of the
\numFaults\ ($\sim$8.8\%) faults.

%% \sfl{}$^{5}$ finds $207$ ($79.6\%$) faults whereas
%% \combpar{5} finds $227$ ($87.3\%$) faults for $k=5$, i.e., \combpar{5}
%% misses $20$ ($7.7\%$) less faults compared to
%% \sfl{}$^{5}$. \sfl{}$^{10}$ finds $217$ ($83.5\%$) faults whereas
%% \combpar{10} finds $237$ ($91.2\%$) for $k=10$, i.e., \combpar{10}
%% misses less $20$ ($7.7\%$) faults than \sfl{}$^{10}$.

%% repeated...
%%  (2) there are few
%% cases where the faulty statements are only found if the approach
%% considers the first $10$ highest ranked faulty classes

% \begin{table*}[t!]
% 	\small
% 	% \setlength{\tabcolsep}{3pt}
% 	\centering
% 	  \begin{tabular}{l|cccc|cccc|c}
% 		\toprule
% 		\multirow{2}{*}{Project}            & \multicolumn{4}{c|}{\sfl{}$^{k}$}  & \multicolumn{4}{c|}{\combpar{k}} & \multirow{2}{*}{\# Faults} \\
%
% 		            & \# $k = 5$ & \% $k = 5$ & \# $k = 10$ & \% $k = 10$
% 					& \# $k = 5$ & \% $k = 5$ & \# $k = 10$ & \% $k = 10$ &  \\
% 		\midrule
% 		 \lang{}         &  55 & 84.6\%  & 55  & 84.6\%
% 		 				& 65 & 100\% & 63 & 96.9\% & 65      \\
% 		\cmath{}           & 85 & 81.7\%  & 89 & 85.6\%
% 						& 93 & 89.4\% & 99 & 95.2\% & 104  \\   %\comments{& 177   &  -}\\
% 		% \closure{}         & 62.41 \%  & 72.18 \%  & 9.77 \%     \\   %\comments{& 350   &  -}\\
% 		\chart{}          & 22 & 84.6\% & 24 & 92.3\%
% 						& 20 & 76.9\% & 22 & 84.6\% & 26    \\
% 		\jtime{}          & 21 & 77.8 \% & 22 & 81.5 \%
% 				& 22 & 81.5\% & 23 & 85.2\% & 27     \\
% 		 \mockito{}      & 24  & 63.2\% & 27 & 71.1\%
% 		 				& 27 & 71.1\% & 30 & 78.9\% & 38 \\\midrule
% 	 Total      & 207  & 79.6\% & 217 & 83.5\%
% 	 				& 227 & 87.3\% & 237 & 91.2\% & 260 \\
% 		\bottomrule
% 	\end{tabular}
%   \caption {Number of faults where at least one of the faulty-statements is in the \sfl{}$^{k}$ and \combpar{k} report}
%   \label{table:fsws}
% \end{table*}

\begin{table}[h]
	\vspace{-0.2cm}
	\small
	\setlength{\tabcolsep}{3pt}
	\centering
\begin{tabular}{l|rr|rr}
		 \toprule
       & \multicolumn{2}{c|}{$k=5$} & \multicolumn{2}{c}{$k=10$} \\
		 Project        &   \sfl{}   & \comb &   \sfl{}   & \comb\\
		\midrule
		 \lang{}         &   84.6   & 100.0 & 84.6 & 96.9  \\
		\cmath{}           & 81.7 & 89.4 & 85.6 & 95.2 \\
		\chart{}           & {\cellcolor{Gray} 84.6} & {\cellcolor{Gray}76.9} & {\cellcolor{Gray}92.3} & {\cellcolor{Gray}84.6}\\
		\jtime{}           & 77.8 & 81.5 & 81.5 & 85.2\\
		\mockito{}           & 63.2 & 71.1 & 71.1 & 78.9\\   \midrule
		Total         & 79.6 & 87.3 & 83.5 & 91.2\\
		\bottomrule
	\end{tabular}
  \caption {Number of faults where at least one of the faulty
    statements appears at the report of the technique. Higher is better.}
  \label{table:fsws}
\vspace{-0.4cm}
\end{table}

\subsubsection{RQ3: \textit{\rqthree}}

%% k=10
\newcommand{\numOutPerformed}{$33$}%
\newcommand{\numEquallyPerformed}{$110$}%


In \textbf{RQ3}, we evaluate the impact of combining \ds{} with \sfl{}
to improve fault localization. The first experiment we ran consists of
measuring the \emph{difference of diagnosis cost} between the two
fault localization techniques. This metric has been previously used in
other
studies~\cite{7390282,ang-perez-van-deursen-rui-2017,Pearson:2017:EIF:3097368.3097441,Perez:2018:LQR:3304889.3304927}.
More specifically, we computed $\Delta{}C = C(\textrm{SFL$^{k}$}) -
C(\textrm{\combpar{k}})$, where $C$ denotes the diagnosis cost and is
obtained by computing the mean position in the ranking of all buggy
statements. $\Delta C <0$ means that \combpar{k} performs worse
compared to its baseline. That could happen if dynamic slicing misses
the faulty statement. $\Delta C=0$ means that the faulty statement
remained in the same position. $\Delta C >0$ means that \combpar{k}
outperformed the baseline, \ie{} the mean position of the faulty
statements is smaller in \combpar{k} compared to that of the
baseline. It is worth noting that we assume perfect bug
understanding~\cite{Parnin:2011:ADT:2001420.2001445}, i.e., if the
developer sees the bug in the ranking, he is able to precisely
determine if it is the faulty one.  Figure \ref{fig:diagnosis}
summarizes the performance of \combpar{k} relative to
\sfl{}$^{k}$. The figure shows a histogram over $\Delta{}C$ for
different values of $k$. Note that (1) the number of cases where the
baseline outperforms \combpar{k} is small (\eg{}, \numOutPerformed\ of
the \numFaults\ faults for $k=10$) and that (2) the number of cases
\combpar{k} outperforms the baseline is significative (\eg{}, $117$ of
the \numFaults\ faults for $k=10$). One
of the following reasons justifies the baseline outperforming
\combpar{k}: (1)
a fault of omission or (2) the test has design limitations which \ds~is
not capable of handling (mentioned in Section~\ref{sec:slicing}).

%% For $k=5$, we observe that in $30$ ($11.5\%$) of the faults the
%% position of faulty statements decreases in the ranking, $97$($37.3\%$)
%% of the faults the position of faulty statements increases in the
%% rankinh whereas $133$ ($51.2\%$) of the faults the position of faulty
%% stamenets remains the same. For $k=10$, we observe an increasing of
%% $20$ cases where the position of faulty statements increases in the
%% ranking $117$ ($45.0\%$).  The number of cases whose faulty statements
%% position remained the same are $110$ ($42.3\%$) and that suffered a
%% decrease in the ranking is $33$ ($12.7\%$).

\begin{figure}[h]
		\centering
		\includegraphics[width=0.415\textwidth]{figures/performance.pdf}
		\vspace{-0.4cm}
		\caption{Delta Cost of Diagnosis ($\Delta{}C$) per $k$}
		\label{fig:diagnosis}
\end{figure}

% \Mar{Sofia, vc. precisa revisar com mais cuidado os dois proximos paragrafos...}

Figure~\ref{fig:boxplot} shows the distributions of $\Delta{}C$ considering all the 
cases where the predicate $\Delta{}C>0$ holds, \ie{}, it additionally excludes from the dataset
the cases of faults where the techniques perform equally for at least one $k$. 
To sum, although we observed a relatively small number of cases where $\Delta{}C$
is negative (\numOutPerformed) and a high number of cases $\Delta{}C$
is zero (\numEquallyPerformed), we conclude that there is a
considerable amount of cases where the use of \combpar{k} would be
very beneficial.
% On the left-hand-side, we show distributions of
% $\Delta{}C$, for different values of $k$, considering all cases where
% the predicate $\Delta{}C\geq{}0$ holds, \ie{}, we excluded the
% \numOutPerformed\ cases where \sfl{}$^{k}$ outperformed \combpar{k}
% (see Figure~\ref{fig:diagnosis}).


% The boxplots on the right-hand-side
% of the figure considers all the cases where the predicate
% $\Delta{}C>0$ holds, \ie{}, it additionally excludes from the dataset
% the cases of faults where the techniques perform equally.\Sof{Needs to be reverted}
% \Mar{sofia,
%   are you sure there are no zeros in this dataset? The min. in this
%   distibution (as per the base of the plot) seems to be 0.}\Sof{Yes, check hangouts} 


\begin{figure}[h]
		\vspace{-0.2cm}
		\centering
		\includegraphics[width=0.45\textwidth]{figures/boxplot.pdf}
		\vspace{-0.4cm}

		\caption{Distributions of $\Delta{}C$ considering all cases where \combpar{k}
        outperformed the baseline}
		\label{fig:boxplot}
		\vspace{-0.2cm}
\end{figure}

In the following, we present statistical analysis of the techniques.
Table~\ref{table:st} reports descriptive statistics about the
distributions of $C$ for the techniques under different values of $k$.
In bold, we present the results of the $227$ faults that do not
comprise the cases that are result of the limitations mentioned before
(test design and fault of omission). The number in parentheses are
relative to the dataset including all data faults; \Sof{improve}note that results
for this dataset are \emph{not} encouraging.

These results show that \ds{}
improves \sfl{} in a mean of $6$ ($3.3\%$ on average) statements on
the average ranking of all faulty statements, when considering
$k=10$.
However, if we only consider
the faults which can benefit from our approach, we can observe that
there is a considerable amount of cases where the approach is actually
successful. For $k=5$, our approach achieves a mean improvement of
$41.1$ ($9.57\%$ on average) statements on the ranking and an
impressive maximum ranking improvement of $3171$ ($96.7\%$ on average)
statements. Whereas for $k=10$, \combpar{k} achieves a mean
improvement of $19$ ($13.35\%$ on average) statements on the ranking
and a maximum ranking improvement of $209$ ($73.7\%$ on average)
statements. Such difference is justified by the fact that entire files
are discarded in \combpar{5}, whose statements are included in the
ranking of \combpar{10}. Consequently, the improvements obtained by
the technique will not be as high if slicing on these additional files
is unable to aggressively and safely discard statements. This also
shows that slicing files, in addition to slicing statements from those
files, is very beneficial for fault localization performance.

\begin{table}[h]
	\tiny
	\centering
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{@{}c|c|c|c|c@{}}
     \toprule
  & \multicolumn{2}{c|}{$k=5$} & \multicolumn{2}{c}{$k=10$} \\
  & SFL  & \comb\       & SFL                & \comb\              \\ \midrule
Mean  & \textbf{779.3} (584.3) & \textbf{738.2} (679.7)   &  \textbf{630.6} (516.4)    & \textbf{611.9} (614.0)   \\ \midrule
Median & \textbf{187.7} (49.4) & \textbf{167.2} (69.3)      & \textbf{167.2} (53.0) & \textbf{107.5} (56.9)\\ \midrule
Variance & \textbf{1218.3} (1223.1) & \textbf{1186.3} (1352.4)  &  \textbf{1058.4} (1148.1) &  \textbf{1048.6} (1294.9) \\ \midrule
Shapiro-Wilk &\makecell{$W$ = 0.67 \\ $p$ = 9.64 x $10^{-15}$} & \makecell{$W$ = 0.66 \\ $p$ = 5.59 x $10^{-15}$} & \makecell{$W$ = 0.63 \\ $p$ = 1.09 x $10^{-15}$} &  \makecell{$W$ = 0.63 \\ $p$ = 8.95 x $10^{-16}$}  \\ \midrule
Friedman & \multicolumn{4}{c}{\makecell{$\chi^{2}$ = 175.18 \\ p-value = 9.71 x $10^{-38}$}} \\
\bottomrule
\end{tabular}
  \caption {Statistical tests for $C$}
  \label{table:st}
\end{table}

Table \ref{table:st} presents the statistics to determine if the
observed results are statistically significant. Shapiro-Wilk tests the
null hypothesis that results are drawn from a normal distribution. The
test is perfomed for all techniques and values of $k$. With $99\%$
confidence, the results tell us that the distributions are not
normal. Given that $C$ is not normally distributed, we used Friedman, a
non-parametric statistical test of hypothesis. The null hypothesis is
that the rankings obtained with all techniques and variants are the
same. With $99\%$ confidence, the results show that the distributions
are distinct. To understand which techniques perform differently,
\ie{}, to answer the question \textit{Does \combpar{k} perform
  differently than \sfl{}$^{k}$}, we performed a Siegel post-hoc
analysis. Figure \ref{fig:performance} reports results. Each square
shows the statistical significance of the difference in diagnostic
accuracy amongst the different techniques--the lighter color and
smaller value of $p$ indicates higher significance. Based on these
results, it is possible to determine with $95\%$ confidence that the
performance of each pair of techniques is different.

\begin{figure}[h]
	\vspace{-1cm}
		\includegraphics[width=0.45\textwidth]{figures/heatmap_nemenyi_result.pdf}
		\caption{Siegel post-hoc analysis results}
		\label{fig:performance}
		\vspace{-0.3cm}
\end{figure}

As the results are statistically significant, it is possible to state
that \combpar{k} has a positive effect on the localization of some
types of bugs. There is indeed a problem on using slicing on fault of
omission bugs that needs further research.  Although, this dataset has
more than $50\%$ of fault of omissions, our simple slicing approach
was capable of improving the ranking of $95$ faults for $k=5$ and
$115$ faults for $k=10$. These may be promissing results from what Automated
Program Repair (APR) techniques may directly benefit to improve the accuracy
of their own techniques.


\subsection{Threats to validity}
%
A potential threat to \textit{external validity} relates to the set of programs used in
our study. When choosing the projects for our study, our aim was to opt for
projects that resemble a general and large-sized application. To reduce
selection bias and facilitate the comparison of our results, we decided to
choose a benchmark that is popular in the community~\cite{just-defects4j-issta2014}.
Our study did not evaluate the faults from the \closure{}
project because of the high CPU cost of running \combpar{k} on it. A potential
threat to \textit{construct validity} relates to the choice of oracle
we used for the Lithium slicer. We found empirically that our choice was able to capture 
the faults for the cases we analyzed.
The main threat to \textit{internal validity} lies in the complexity of several of the tools
used in our experiments, most notably the Lithium toolset, the SFL diagnosis tool,
and the implementation of \comb{}. To mitigate this threat, we
carefully inspected our code and looked for discrepancies and
incoherences in our results.

%
\section{Related Work}

Model-based diagnosis is a well-known approach that has been proposed by the DX
community, a sub-field of
AI~\cite{REITER198757,wotawa2002relationship,DEKLEER200325}. Applications of
model-based diagnosis to localize software faults have demonstrated that it can
be framed as dynamic
slicing~\cite{Mayer:2008:EMM:1642931.1642950,mayer2008prioritising,nica2013use}.
Not many interesting techniques have been pursued due to scalability
limitations. We have tackled this using the program slicing technique ---
critical slicing~\cite{DeMillo:1996:CSS:229000.226310} --- for its simplicity
and generality.

Dynamic Slicing has found its main application in fault
localization~\cite{Agrawal:1990:DPS:93542.93576}, e.g.,  on the minimization of
failing tests through the identification of code components that do not
contribute to the fault revelation. Research in this area has mainly focused on
slicing code
efficiently~\cite{Wang:2008:DSJ:1330017.1330021,Wang:2004:UCB:998675.999455} and
locate and contour faults of
omission~\cite{Zhang:2007:TLE:1250734.1250782,Lin:2018:BDE:3238147.3238163}.

Statistics-based techniques (e.g., ~\cite{Pearson:2017:EIF:3097368.3097441}) are
popular automated fault localization techniques. Spectrum-based fault
localization (\sfl{}) is amongst the most common statistical fault localization
technique used to rank faulty components
~\cite{7390282,DBLP:conf/kbse/JonesH05,DBLP:journals/smr/LuciaLJTB14,DBLP:journals/jss/AbreuZGG09}.
This technique uses tests coverage information to rank program's components.

Prior work has investigated the \ds{}-\sfl{}
combination~\cite{Wotawa:2010:FLB:1848650.1849235,Alves:2011:FUD:2190078.2190115,DBLP:conf/ecai/HoferW12,lei-mao-dai-wang-2012,slicing-sfl-repair,christi2018reduce}.
Overall, these studies reported promissing results but based their conclusions
on a very small group of (mostly artificial) faults and using different
techniques. In this
study, a larger set of faults is evaluated and a more rigorous experimental
methodology.
In contrast with other studies on this combination,
our approach is the first using Critical Slicing to minimize test cases
and leveraging \sfl{} rank to choose and minimize the top-$k$ of
highest-ranked faulty classes.



% %
\section{Conclusions and Future Work}
\label{sec:conc}
%

Several approaches have been proposed in the literature to reduce debugging
costs through automated software fault diagnosis with the goal of
improving productivity in software developement. In this domain, Dynamic Slicing (\ds)
and Spectrum-based Fault Localization (\sfl) are very popular fault diagnosis
techniques and normally seen as complementary. This paper reports the outcome of
a study using real-world applications with the goal of demystifying the impact
of combining \ds~with \sfl. The results of our empirical study show that, in
practice, \ds~rarely misses faulty statements $9\%$ ($23$ misses in $260$ cases) and that
the \ds-\sfl~combination, coined as \combpar{k},
improves the diagnostic accuracy up to $73.7\%$ ($13.4\%$ on average).

Despite tacit opinions of the research community about the usefulness
of \ds~in automated debugging, we found te \ds{}-\sfl{} combination
practical (as per the relatively low number of misses) and effective
(as per the improvements of $\Delta(C)$ on the relevant cases), and
encourage new \sfl~techniques to be evaluated against that
optimization.  Future work includes 1)~improving our oracle heuristics
to handle more exceptions, 2)~conducting experiments in more \dfj{}
faults and other datasets, and 3)~handling cases of faults of
omission.

The dataset and tools (Lithium slicer and \comb{}) will be made
available once double-blind requirements are lifted.

%% the
%% cases which harm the cost of diagnosis, such as, faults of omission,
%% test design problems and the cases which the buggy-class is not in the
%% top-$k$ of highest-rank faulty classes.

%
%\section*{Acknowledgments}
%include only after review.
{
  \small
  \balance
  \bibliographystyle{named}
  \bibliography{paper}
}

\end{document}

%%  LocalWords:  SFL
